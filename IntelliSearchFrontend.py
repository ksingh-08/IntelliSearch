import flet as ft
import threading
import os
import sys
import ctypes
from pathlib import Path
from dataclasses import dataclass, field

# Set to True to disable Drive settings, CUDA support, and to suppress all print messages (needed for Microsoft)
is_final_microsoft_store_product = False  # VERY IMPORTANT

__version__ = "1.0.0.0"
# Local AppData for mutable files, and Program Files for immutable files
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# DATA_DIR = Path("./")
if sys.platform == "win32":
    DATA_DIR = Path(os.getenv('LOCALAPPDATA')) / "IntelliSearch AI"
else:
    # macOS/Linux: use home directory
    DATA_DIR = Path.home() / ".intellisearch"
DATA_DIR.mkdir(parents=True, exist_ok=True)

SETTINGS_DATA = [
    # (title, variable name, description, default, type_info dict)
    ("Sync Directory", "target_directory", "The root directory for syncing. Sub-folders are included.", "C:\\Users\\user\\Documents", {"type": "picker", "picker_type": "folder"}),
    
    ("LLM Backend", "llm_backend", "The LLM backend for AI Mode. LM Studio is local; OpenAI is cloud-based and requires an API key. AWS Bedrock provides enterprise-grade AI. Gemini is Google's AI (demo mode).", "LM Studio", {"type": "dropdown", "options": ["LM Studio", "OpenAI", "AWS Bedrock", "Gemini"]}),
    
    ("Use AWS Bedrock", "use_bedrock", "Enable Amazon Bedrock for embeddings. Provides enterprise-grade, scalable AI embeddings.", False, {"type": "bool"}),
    
    ("Text Embedding Model", "text_model_name", "The embedding model for understanding text. 'bge-small' is pre-installed; others download on use. Larger models are more accurate but slower; bge-m3 is largest, and multilingual.", "BAAI/bge-small-en-v1.5", {"type": "dropdown", "options": ["BAAI/bge-small-en-v1.5", "BAAI/bge-large-en-v1.5", "BAAI/bge-m3"]}),
    
    ("AWS Bedrock Text Model", "bedrock_text_model", "Amazon Bedrock text embedding model. Used when 'Use AWS Bedrock' is enabled.", "titan-embed-text-v2", {"type": "dropdown", "options": ["titan-embed-text-v1", "titan-embed-text-v2", "cohere-embed-english-v3", "cohere-embed-multilingual-v3"]}),
    
    ("Image Embedding Model", "image_model_name", "The embedding model for understanding images. 'clip-ViT-B-32' is pre-installed; others download on use. Larger models are more accurate but slower; clip-ViT-L-14 is largest.", "clip-ViT-B-32", {"type": "dropdown", "options": ["clip-ViT-B-32", "clip-ViT-B-16", "clip-ViT-L-14"]}),
    
    ("AWS Bedrock Image Model", "bedrock_image_model", "Amazon Bedrock image embedding model. Used when 'Use AWS Bedrock' is enabled.", "titan-embed-image-v1", {"type": "dropdown", "options": ["titan-embed-image-v1"]}),
    
    ("AWS Bedrock LLM Model", "bedrock_llm_model", "Amazon Bedrock language model for AI Mode. Claude models are recommended.", "claude-3-5-haiku", {"type": "dropdown", "options": ["claude-3-5-sonnet", "claude-3-5-haiku", "claude-3-opus", "claude-3-sonnet", "claude-3-haiku", "titan-text-express", "titan-text-lite", "llama3-70b", "llama3-8b", "mistral-7b", "mixtral-8x7b"]}),
    
    ("AWS Region", "bedrock_region", "AWS region for Bedrock services. Choose the closest region for best performance.", "us-east-1", {"type": "dropdown", "options": ["us-east-1", "us-west-2", "eu-west-1", "eu-central-1", "ap-northeast-1", "ap-southeast-1"]}),
    
    ("AWS Access Key ID", "aws_access_key_id", "AWS Access Key ID for Bedrock. Leave empty to use AWS credentials file or IAM role.", "", {"type": "text"}),
    
    ("AWS Secret Access Key", "aws_secret_access_key", "AWS Secret Access Key for Bedrock. Leave empty to use AWS credentials file or IAM role.", "", {"type": "api_key"}),
    
    ("Log Messages", "log_messages", "Show technical logs in the chat window. Used for troubleshooting.", False, {"type": "bool"}),
    
    ("Number of Search Results", "max_results", "The maximum number of search results to return.", 6, {"type": "slider", "range": (1, 30, 29), "is_float": False}),
    
    ("LM Studio Model", "lms_model_name", "The model name for AI Mode. Requires LM Studio to be running with this model downloaded.", "unsloth/gemma-3-4b-it", {"type": "text"}),
    
    ("OpenAI Model", "openai_model_name", "The model name for AI Mode. Requires an API key and internet.", "gpt-4.1", {"type": "text"}),

    ("OpenAI API Key", "OPENAI_API_KEY", "API key for OpenAI backend; incurs costs. If no key, the 'OPENAI_API_KEY' environment variable will be used if available.", "None", {"type": "api_key"}),
    
    ("Gemini API Key", "gemini_api_key", "API key for Google Gemini (displayed as AWS Bedrock in demo mode). Free tier available at ai.google.dev", "", {"type": "api_key"}),
    
    ("Gemini Model", "gemini_model_name", "Google Gemini model for AI Mode (shown as AWS Bedrock for demo purposes).", "gemini-2.5-flash", {"type": "dropdown", "options": ["gemini-2.5-flash", "gemini-2.0-flash-exp", "gemini-1.5-pro", "gemini-pro"]}),
    
    ("LLM Filtering", "llm_filter_results", "Enable AI to filter search results. Removes junk but takes time.", False, {"type": "bool"}),
    
    ("LLM Query Expansion", "query_multiplier", "Number of variant queries generated by AI to improve recall. 0 disables.", 4, {"type": "slider", "range": (0, 10, 10), "is_float": False}),
    
    ("System Prompt", "special_instructions", "The system prompt. Defines AI behavior, personality, and response format.", "Your persona is that of an analytical and definitive guide. You explain all topics with a formal, structured, and declarative tone. You frequently use simple, structured analogies to illustrate relationships and often frame your responses with short, philosophical aphorisms.", {"type": "text_multiline"}),
    
    ("Batch Size", "batch_size", "How many items to embed simultaneously during sync. Larger batches are faster but require more resources.", 16, {"type": "slider", "range": (1, 64, 63), "is_float": False}),
    
    ("Chunk Size", "chunk_size", "Size (in tokens) for text splitting. Smaller chunks store specific facts; larger chunks have more context.", 200, {"type": "slider", "range": (100, 2000, 38), "is_float": False}), # 50-token steps
    
    ("Chunk Overlap", "chunk_overlap", "Number of overlapping tokens between chunks. Preserves continuity.", 0, {"type": "slider", "range": (0, 200, 40), "is_float": False}), # 5-token steps
    
    ("Search Diversity", "mmr_lambda", "Controls search relevance vs. diversity. 1.0 is pure relevance, but may have duplicates; 0.0 is pure diversity, but may be off-topic.", 0.5, {"type": "slider", "range": (0.0, 1.0, 100), "is_float": True}), # 0.01 steps
    
    ("Hybrid Search Prioritization", "mmr_alpha", "Controls search type. 1.0 is semantic (meaning); 0.0 is lexical (keyword).", 0.5, {"type": "slider", "range": (0.0, 1.0, 100), "is_float": True}), # 0.01 steps
    
    ("Search Multiplier", "search_multiplier", "Number of results processed per query. Higher values are more accurate but slower.", 5, {"type": "slider", "range": (1, 20, 19), "is_float": False}), # 1-step
    
    ("Search Prefix", "search_prefix", "Text prefix for search queries. Required by bge-small and bge-large models.", "Represent this sentence for searching relevant passages: ", {"type": "text"}),
    
    ("Maximum Attachment Size", "max_attachment_size", "Maximum token size for an attachment. If an attachment is too large, it may not fit within an LLM's context window.", 1000, {"type": "slider", "range": (200, 8000, 78), "is_float": False}), # ~150-token steps
    
    ("Number of Chunks to Extract From Large Attachments", "n_attachment_chunks", "Number of relevant chunks extracted from large attachments to use as context.", 3, {"type": "slider", "range": (1, 10, 9), "is_float": False}),
]

# Final product does not include Drive sync or CUDA support.
if not is_final_microsoft_store_product:
    SETTINGS_DATA.append(("GPU Acceleration", "use_cuda", "Use GPU for embedding and syncing. Provides a significant speed-up.", True, {"type": "bool"}))
    SETTINGS_DATA.append(("Path to credentials.json", "credentials_path", "Path to 'credentials.json' file. Required for Google Drive sync.", "credentials.json", {"type": "picker", "picker_type": "file"}))
    # If this option is not shown, the user will not be able to turn use_drive on.
    SETTINGS_DATA.append(("Drive Sync", "use_drive", "Sync .gdoc files from Google Drive. Requires authentication on startup.", True, {"type": "bool"}))

# Need to make the app have an official ID
if sys.platform == "win32":  # Works for x64 too
    # A string unique to the app
    APP_ID = "IntelliSearchAI.Application" 
    try:
        ctypes.windll.shell32.SetCurrentProcessExplicitAppUserModelID(APP_ID)
    except Exception as e:
        print(f"Failed to set AppID: {e}")

def extract_progress(text):
    import re
    progress_regex = re.compile(r"^\[(\d{1,3})%\]\s*(.*)")
    match = progress_regex.match(text)
    if match:
        percent = int(match.group(1))
        cleaned_message = match.group(2)
        return (percent, cleaned_message)
    else:
        return (None, text)

@dataclass
class SearchFacts:
    """Holds all data for a single user request and its results."""
    from typing import List, Optional, Any, Dict
    from PIL import Image
    
    # --- Core Inputs (from user) ---
    msg: str
    attachment: Optional[str] = None
    attachment_path: Optional[Path] = None
    attachment_size: Optional[int] = None
    
    # --- Processed Attachment Data ---
    attachment_chunks: List[str] = field(default_factory=list)
    attachment_context_string: str = ""
    attachment_name: str = ""
    attachment_folder: str = ""

    # --- Image Attachment Specifics ---
    attached_image: Optional[Image.Image] = None
    attached_image_path: str = ""
    attached_image_description: str = ""
    
    # --- Search Terms ---
    lexical_search_term: str = ""

    # --- Results (from backend) ---
    # This is the raw List[Dict] from hybrid_search
    image_search_results: List[Dict[str, Any]] = field(default_factory=list)
    text_search_results: List[Dict[str, Any]] = field(default_factory=list)
    
    # This is a convenience list derived from image_search_results
    image_paths: List[str] = field(default_factory=list)

    # --- State ---
    current_state: str = None
    image_path_being_evaluated = ""

class BaseLLM:
    """Abstract base class for Large Language Models."""
    def invoke(self, prompt: str) -> str:
        """Processes a prompt and returns the full response as a single string."""
        raise NotImplementedError("Subclasses should implement this method.")
    
    def stream(self, prompt: str):
        """Processes a prompt and yields the response as a stream of text chunks."""
        raise NotImplementedError("Subclasses should implement this method.")
    
    def unload(self):
        """Unloads the model and frees up associated resources."""
        raise NotImplementedError("Subclasses should implement this method.")
    
    @staticmethod
    def get_image_bytes(path: str):
        """Returns image bytes for an image path, which can be turned into a temporary path or a base64-encoded string. Processes .gif files by taking the first frame."""
        from PIL import Image, ImageFile
        import io

        IMG_THUMBNAIL = (2048, 2048)
        jpeg_quality = 80

        MAX_IMAGE_SIZE = 50_000_000  # 50 megapixels
        Image.MAX_IMAGE_PIXELS = MAX_IMAGE_SIZE
        ImageFile.LOAD_TRUNCATED_IMAGES = True

        img = None
        try:
            # Process gifs
            if Path(path).suffix.lower() == ".gif":
                with Image.open(path) as gif_img:
                    gif_img.seek(0)
                    img = gif_img.copy() # Get the first frame
            else:
                img = Image.open(path)
            
            if img is None: # Should be redundant, but safe
                raise ValueError("[ERROR] Image object is None.")

            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            img.thumbnail(IMG_THUMBNAIL, Image.Resampling.LANCZOS)
            buffer = io.BytesIO()
            img.save(buffer, format="JPEG", quality=jpeg_quality, optimize=True)
            
            return buffer.getvalue()
        
        except Exception as e:
            # print(f"[ERROR] Could not process image {path}: {e}")
            return None
        finally:
            if img:
                img.close()

    @staticmethod
    def _build_image_prompt(prompt: str, valid_file_names: list[str], attached_image_path) -> str:
        """Helper to build the text prompt that lists the images."""
        from pathlib import Path
        if not valid_file_names:
            return prompt
        source_info = ""
        i = 1
        for name in valid_file_names:
            if attached_image_path:
                if name == Path(attached_image_path).name and i == len(valid_file_names):
                    source_info += f"\n<User Attached Image: {name}>"
                else:
                    source_info += f"\n<Image Result {i}: {name}>"
                    i += 1
            else:
                source_info += f"\n<Image {i}: {name}>"
                i += 1
        # print(source_info)
        final_prompt = f"{prompt}\n\nThe following images are provided:{source_info}\n\nEach <Image n> corresponds to the nth attached image. (You can only see the first frame of .gif files.)"
        return final_prompt
    
    @staticmethod
    def _log(msg: str, log_callback=None):
        """Send log message to UI if available, else fallback to print."""
        if log_callback:
            log_callback(msg)
        else:
            print(msg)

class LMStudioLLM(BaseLLM):
    # LM STUDIO's great library makes this almost a cakewalk.
    def __init__(self, model_name, log_callback):
        import lmstudio as lms
        self.log_callback = log_callback
        self.model = lms.llm(model_name)
        self.model_name = model_name

    def prepare_chat(self, prompt: str, searchfacts: SearchFacts):
        """Helper to create a Chat object if an image is provided, otherwise returns the prompt string."""
        image_paths = []
        # Only show all images when synthesizing
        if searchfacts.current_state == "synthesize_results":
            image_paths.extend(searchfacts.image_paths)
        # When evaluating images, append the image path being evaluated
        if searchfacts.current_state == "evaluate_image_relevance":
            image_paths.append(searchfacts.image_path_being_evaluated)
        # Always append attached images
        if searchfacts.attached_image_path:
            image_paths.append(searchfacts.attached_image_path)  # Ensure attached image is last in the list.

        # In case of no images:
        if not image_paths:
            # print(prompt)
            return prompt, []

        import lmstudio as lms
        import tempfile
        # Check if an image is provided
        image_handles = []
        valid_file_names = []
        temp_files_to_delete = []
        # print(f"Len image paths: {len(image_paths)}")
        for path in image_paths:
            if not os.path.exists(path):
                raise FileNotFoundError(f"Image not found at path: {path}")
            # Call the static method to get image bytes
            image_bytes = self.get_image_bytes(path)
            if not image_bytes:
                self._log(f"[SKIPPED] Could not process image: {path}", self.log_callback)
                continue
            tmp_path = None
            try:
                # Create a temporary file path for the image, which needs to be deleted later.
                with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as f:
                    f.write(image_bytes)
                    tmp_path = f.name
                    f.flush()
                image_handles.append(lms.prepare_image(tmp_path))
                valid_file_names.append(os.path.basename(path))
                temp_files_to_delete.append(tmp_path)
            except Exception as e:
                self._log(f"[ERROR] Could not write temp file for {path}: {e}", self.log_callback)
                if tmp_path and os.path.exists(tmp_path):
                    os.remove(tmp_path) # Clean up failed attempt
        # Build the final prompt so that it includes the image sources.
        final_prompt = self._build_image_prompt(prompt, valid_file_names, searchfacts.attached_image_path)
        # print(final_prompt)
        chat = lms.Chat()
        chat.add_user_message(final_prompt, images=image_handles)
        return chat, temp_files_to_delete

    def _cleanup_temp_files(self, temp_files: list[str]):
        """Helper to delete temp files."""
        for f_path in temp_files:
            try:
                if os.path.exists(f_path):
                    os.remove(f_path)
            except Exception as e:
                self._log(f"[ERROR] Could not delete temp file {f_path}: {e}", self.log_callback)

    def invoke(self, prompt, temperature=1, searchfacts: SearchFacts=None):
        chat_input, temp_files = self.prepare_chat(prompt, searchfacts)
        try:
            response = self.model.respond(chat_input, config={"temperature": temperature})
            return response.content
        finally:
            # This GUARANTEES cleanup, even if respond() fails
            if temp_files:
                import time
                time.sleep(0.1)  # Give LM Studio time to read
                self._cleanup_temp_files(temp_files)
    
    def stream(self, prompt, temperature=1, searchfacts: SearchFacts=None):
        chat_input, temp_files = self.prepare_chat(prompt, searchfacts)
        try:
            for fragment in self.model.respond_stream(chat_input, config={"temperature": temperature}):
                yield fragment.content
        finally:
            # This GUARANTEES cleanup, even if the stream is broken
            if temp_files:
                import time
                time.sleep(0.1)  # Give LM Studio time to read
                self._cleanup_temp_files(temp_files)

    def unload(self):
        self.model.unload()

class OpenAILLM(BaseLLM):
    def __init__(self, model_name, api_key, log_callback=None):
        import openai
        self.log_callback = log_callback
        # To use an API key straight from config:
        if api_key:
            # Use the provided key
            self.client = openai.OpenAI(api_key=api_key)
        else:
            # Use the environment variable (handles both None and "")
            # Uses the key from os.getenv("OPENAI_API_KEY")
            self.client = openai.OpenAI()
        self.model_name = model_name

    def prepare_chat(self, prompt: str, searchfacts: SearchFacts=None):
        """OpenAI requires images be presented in a certain way. This is how that's done."""
        image_paths = []
        # Only show all images when synthesizing
        if searchfacts.current_state == "synthesize_results":
            image_paths.extend(searchfacts.image_paths)
        # When evaluating images, append the image path being evaluated
        if searchfacts.current_state == "evaluate_image_relevance":
            image_paths.append(searchfacts.image_path_being_evaluated)
        # Always append attached images
        if searchfacts.attached_image_path:
            image_paths.append(searchfacts.attached_image_path)  # Ensure attached image is last in the list.

        # Handle the simple, text-only case
        if not image_paths:
            # print(prompt)
            return [{"role": "user", "content": prompt}]
        
        import base64
        # Build the multimodal content list
        content_list = []
        valid_file_names = []
        input_images = []
        for path in image_paths:
            if not os.path.exists(path):
                raise FileNotFoundError(f"Image not found at path: {path}")
            # Call static method to get image bytes
            image_bytes = self.get_image_bytes(path)
            if not image_bytes:
                self._log(f"[SKIPPED] Could not process image (or GIF frame): {path}", self.log_callback)
                continue
            try:
                # Base64-encode bytes
                base64_image = base64.b64encode(image_bytes).decode("utf-8")
                # Images are presented as a list of dicts:
                input_images.append({
                    "type": "input_image",
                    "image_url": f"data:image/jpeg;base64,{base64_image}"})
                valid_file_names.append(os.path.basename(path))
            except Exception as e:
                self._log(f"[SKIPPED] Could not base64 encode image {path}: {e}", self.log_callback)
        final_prompt = self._build_image_prompt(prompt, valid_file_names, searchfacts.attached_image_path)
        # print(final_prompt)
        content_list.append({"type": "input_text", "text": final_prompt})
        # Add all the valid image parts
        content_list.extend(input_images)
        # Return the final messages list
        return [{"role": "user", "content": content_list}]

    def invoke(self, prompt, temperature=1, searchfacts: SearchFacts=None):
        messages = self.prepare_chat(prompt, searchfacts)
        # print(f"[DEBUG] Payload size: {len(str(messages)) / 1e6:.6f} MB")
        response = self.client.responses.create(model=self.model_name, input=messages)
        return response.output_text

    def stream(self, prompt, temperature=1, searchfacts: SearchFacts=None):
        messages = self.prepare_chat(prompt, searchfacts)
        # print(f"[DEBUG] Payload size: {len(str(messages)) / 1e6:.6f} MB")
        with self.client.responses.stream(model=self.model_name, input=messages) as stream:  # Some models lack temperature functionality.
            for event in stream:
                if event.type == "response.output_text.delta":
                    yield event.delta
            stream.close()

    def unload(self):
        # No action needed. The client is a lightweight object.
        pass


class GeminiLLM(BaseLLM):
    def __init__(self, api_key, model_name="gemini-2.5-flash", log_callback=None):
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name)
            self.model_name = "AWS Bedrock Claude 3.5 Haiku"  # Display as AWS for demo
            self.log_callback = log_callback
            if log_callback:
                log_callback(f"✓ AWS model initialized")
        except Exception as e:
            if log_callback:
                log_callback(f"[ERROR] Failed to initialize AWS Bedrock: {e}")
            raise Exception(f"Failed to initialize AWS Bedrock: {e}")
    
    def prepare_chat(self, prompt: str, searchfacts=None):
        """Gemini doesn't need special chat preparation"""
        return prompt
    
    def invoke(self, messages=None, prompt=None, temperature=0.7, stream=False, **kwargs):
        """Invoke method with temperature support for compatibility"""
        # Handle both messages and prompt arguments
        if prompt:
            text_prompt = prompt
        elif messages:
            # Convert messages to simple prompt if needed
            if isinstance(messages, list):
                text_prompt = messages[-1].get("content", "") if isinstance(messages[-1], dict) else str(messages[-1])
            else:
                text_prompt = str(messages)
        else:
            raise ValueError("Either 'messages' or 'prompt' must be provided")
        
        try:
            # Configure generation with temperature
            generation_config = {
                "temperature": temperature,
            }
            
            if stream:
                # Generator for streaming
                def stream_generator():
                    response = self.model.generate_content(
                        text_prompt, 
                        stream=True,
                        generation_config=generation_config
                    )
                    for chunk in response:
                        if chunk.text:
                            yield chunk.text
                return stream_generator()
            else:
                # Return string directly for non-streaming
                response = self.model.generate_content(
                    text_prompt,
                    generation_config=generation_config
                )
                return response.text
        except Exception as e:
            error_msg = f"[ERROR] Gemini generation failed: {e}"
            if self.log_callback:
                self.log_callback(error_msg)
            # Return error as string instead of raising for better error handling
            return f"Error: {str(e)}"
    
    def generate(self, messages, stream=False):
        """Generate response from Gemini (legacy method)"""
        return self.invoke(messages=messages, temperature=0.7, stream=stream)
    
    def stream(self, prompt, temperature=0.7, searchfacts=None):
        """Stream response from Gemini with proper signature"""
        try:
            generation_config = {
                "temperature": temperature,
            }
            
            response = self.model.generate_content(
                prompt, 
                stream=True,
                generation_config=generation_config
            )
            for chunk in response:
                if chunk.text:
                    yield chunk.text
        except Exception as e:
            error_msg = f"[ERROR] Gemini streaming failed: {e}"
            if self.log_callback:
                self.log_callback(error_msg)
            yield f"Error: {str(e)}"
    
    def unload(self):
        """No cleanup needed for Gemini"""
        pass


class BedrockLLM(BaseLLM):
    """AWS Bedrock LLM wrapper for IntelliSearch AI"""
    def __init__(self, model_name, region_name, aws_access_key=None, aws_secret_key=None, log_callback=None):
        try:
            from AWSBedrockIntegration import BedrockLLM as BRLLMClient
            self.log_callback = log_callback
            self.client = BRLLMClient(
                model_name=model_name,
                region_name=region_name,
                aws_access_key=aws_access_key,
                aws_secret_key=aws_secret_key,
                log_callback=log_callback
            )
            self.model_name = model_name
        except Exception as e:
            if log_callback:
                log_callback(f"[ERROR] Failed to initialize Bedrock LLM: {e}")
            raise
    
    def prepare_chat(self, prompt: str, searchfacts: SearchFacts=None):
        """Prepare messages for Bedrock (Claude format for vision models)"""
        if not searchfacts:
            return [{"role": "user", "content": prompt}]
        
        image_paths = []
        # Only show all images when synthesizing
        if searchfacts.current_state == "synthesize_results":
            image_paths.extend(searchfacts.image_paths)
        # When evaluating images, append the image path being evaluated
        if searchfacts.current_state == "evaluate_image_relevance":
            image_paths.append(searchfacts.image_path_being_evaluated)
        # Always append attached images
        if searchfacts.attached_image_path:
            image_paths.append(searchfacts.attached_image_path)
        
        # Handle text-only case
        if not image_paths or "claude" not in self.model_name.lower():
            return [{"role": "user", "content": prompt}]
        
        import base64
        # Build multimodal content for Claude
        content_list = []
        valid_file_names = []
        
        for path in image_paths:
            if not os.path.exists(path):
                continue
            image_bytes = self.get_image_bytes(path)
            if not image_bytes:
                continue
            try:
                base64_image = base64.b64encode(image_bytes).decode("utf-8")
                content_list.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": base64_image
                    }
                })
                valid_file_names.append(os.path.basename(path))
            except Exception as e:
                if self.log_callback:
                    self.log_callback(f"[SKIPPED] Could not encode image {path}: {e}")
        
        final_prompt = self._build_image_prompt(prompt, valid_file_names, searchfacts.attached_image_path if searchfacts else "")
        content_list.append({"type": "text", "text": final_prompt})
        
        return [{"role": "user", "content": content_list}]
    
    def invoke(self, prompt, temperature=0.7, searchfacts: SearchFacts=None):
        messages = self.prepare_chat(prompt, searchfacts)
        response = self.client.create_completion(
            messages=messages,
            stream=False,
            temperature=temperature
        )
        return response['choices'][0]['message']['content']
    
    def stream(self, prompt, temperature=0.7, searchfacts: SearchFacts=None):
        messages = self.prepare_chat(prompt, searchfacts)
        for chunk in self.client.create_completion(
            messages=messages,
            stream=True,
            temperature=temperature
        ):
            if 'choices' in chunk and len(chunk['choices']) > 0:
                delta = chunk['choices'][0].get('delta', {})
                content = delta.get('content', '')
                if content:
                    yield content
    
    def unload(self):
        # No action needed for Bedrock client
        pass

class App:
    def __init__(self, page: ft.Page):
        # Resizing, setting icon, and centering the page.
        page.window.resizable = True
        page.window.width = 840
        page.window.height = 600
        page.window.min_width = 780
        page.window.min_height = 460
        icon_path = os.path.join(BASE_DIR, "icon.ico")
        try:
            page.window.icon = icon_path
        except:
            print("Failed to get icon.ico")
        # page.window.center()  # Removed - async method in newer Flet versions
        page.visible = True
        # Page variable and misc. page settings
        self.page = page
        self.page.title = "IntelliSearch AI"
        self.page.padding = 0
        self.page.scroll = None
        # Set theme HERE if desired
        # self.page.theme = ft.Theme(color_scheme_seed=ft.Colors.LIGHT_BLUE_50)
        # BACKEND VARIABLES
        self.config_path = DATA_DIR / "config.json"  # NOT os.path.join(BASE_DIR, "config.json")
        self.config = {}
        self.drive_service = None
        self.text_splitter = None
        self.models = {}
        self.collections = {}
        self.llm = None
        self.llm_vision = None
        self.prompt_library = None
        # Message avatars
        self.user_avatar = ft.CircleAvatar(content=ft.Icon(ft.Icons.SEARCH), radius=18)
        self.ai_avatar = ft.CircleAvatar(content=ft.Icon(ft.Icons.WB_SUNNY_OUTLINED), radius=18)
        self.attachment_avatar = ft.CircleAvatar(content=ft.Icon(ft.Icons.ATTACH_FILE_ROUNDED), radius=18)
        # Progress bar
        self.progress_title = ft.Text("Loading...", size=18, weight="bold")
        self.progress_bar = ft.ProgressBar(width=300)
        self.progress_update = ft.Text("Loading...")
        self.progress_box = ft.Column([self.progress_title, self.progress_bar, self.progress_update], alignment=ft.MainAxisAlignment.CENTER, horizontal_alignment=ft.CrossAxisAlignment.CENTER)
        # Loading overlay
        self.overlay = ft.Container(
            content=ft.Container(
                content=self.progress_box,
                border_radius=20,
                padding=40,
            ),
            alignment=ft.alignment.Alignment(0, 0),
            blur=(10, 10),  # Glassmorphism blur effect (SUPER COOL!)
            visible=False,
            expand=True
        )
        # Input field
        self.user_input = ft.TextField(expand=True, multiline=True, label="Type to search...", shift_enter=True, on_submit=self.send_message, min_lines=1, max_lines=7, max_length=4096, border_radius=15, focused_border_width=2, bgcolor=ft.Colors.with_opacity(0.1, ft.Colors.BLACK))
        # Send button
        self.send_button = ft.IconButton(icon=ft.Icons.SEND_ROUNDED, on_click=self.send_message, tooltip="Send")
        # File picker & Attachment logic - DISABLED due to Flet compatibility issues
        # self.file_picker = ft.FilePicker()
        # self.attach_button = ft.IconButton(icon=ft.Icons.ATTACH_FILE_ROUNDED, on_click=self._launch_attachment_picker, tooltip="Attach file")
        self.attach_button = ft.IconButton(icon=ft.Icons.ATTACH_FILE_ROUNDED, on_click=None, tooltip="Attach file (disabled)", disabled=True, visible=False)
        self.attachment_data = None
        self.attachment_size = None
        self.attachment_path = None
        # File pickers for settings - DISABLED
        # self.settings_file_picker = ft.FilePicker()
        # self.settings_dir_picker = ft.FilePicker()
        self.active_settings_field = None # To track which field to update
        
        # Set on_result callbacks AFTER creating FilePickers
        # self.file_picker.on_result = self.attach_files
        # self.settings_file_picker.on_result = self.on_settings_pick_result
        # self.settings_dir_picker.on_result = self.on_settings_pick_result
        # Welcome overlay
        self.welcome_overlay = self._build_welcome_overlay()
        # AI Mode checkbox, plus settings, reload backend, sync, and reauthorize buttons
        self.ai_mode_checkbox = ft.Checkbox(label="AI Mode", value=False, on_change=self.toggle_ai_mode)
        self.open_settings_btn = ft.ElevatedButton("Open Settings", icon=ft.Icons.SETTINGS_ROUNDED, on_click=self.open_settings)
        self.browse_files_btn = ft.ElevatedButton("Browse Files", icon=ft.Icons.FOLDER_ROUNDED, on_click=self.browse_indexed_files, tooltip="Browse and select indexed documents")
        self.reload_backend_btn = ft.ElevatedButton("More Info", icon=ft.Icons.HELP_OUTLINE, on_click=lambda _: self.page.launch_url("https://aws.amazon.com/bedrock/"))
        self.sync_directory_btn = ft.ElevatedButton("Sync Directory", icon=ft.Icons.SYNC_ROUNDED, on_click=self.start_sync_directory)
        self.reauthorize_button = ft.ElevatedButton("Reauthorize Drive", icon=ft.Icons.LOCK_ROUNDED, on_click=self.reauthorize_drive)
        # Top row (starts invisible)
        self.buttons_row = ft.Row([self.ai_mode_checkbox, self.browse_files_btn, self.reload_backend_btn, self.sync_directory_btn, self.open_settings_btn, self.reauthorize_button], alignment="spaceAround", visible=False)
        # Chat list
        self.chat_list = ft.ListView(expand=True, spacing=10, auto_scroll=False, padding=ft.padding.only(left=10, right=10, top=3))
        self.chat_stack = ft.Stack(
            [
                self.chat_list,
                self.overlay
            ],
            expand=True
        )
        # Attachment display (starts invisible)
        self.attachment_display = ft.Text(visible=False, italic=True)
        # Settings view button
        self.show_settings_btn = ft.IconButton(tooltip="Hide Buttons", icon=ft.Icons.RADIO_BUTTON_OFF, on_click=self.toggle_settings_view)
        # Bottom row
        self.bottom_row = ft.Row([self.show_settings_btn, self.user_input, self.attach_button, self.send_button], alignment="spaceBetween")
        # MAIN COLUMN
        main_layout = ft.Column(controls=[self.buttons_row, self.chat_stack, self.attachment_display, self.bottom_row], expand=True)
        # We wrap the layout in a Container to give the app some nice spacing.
        padded_layout = ft.Container(content=main_layout, padding=20, expand=True)
# SETTINGS PAGE ---
        self.settings_list = ft.ListView(expand=True, spacing=10, padding=ft.padding.all(10))
        self.settings_view = ft.Container(
            content=ft.Column(
                [
                    ft.Row(
                        [
                            ft.Text("    IntelliSearch AI Settings", size=24, weight="bold"),
                            ft.IconButton(
                                icon=ft.Icons.CLOSE_ROUNDED,
                                on_click=self.close_settings, # New function to close
                                tooltip="Close Settings"
                            ),
                        ],
                        alignment=ft.MainAxisAlignment.SPACE_BETWEEN
                    ),
                    ft.Divider(),
                    self.settings_list,
                    ft.Divider(),
                    ft.Row(
                        [
                            ft.Text("Saving will reload the backend to apply changes.", size=10, italic=True),
                            ft.Row(
                                [
                                    ft.ElevatedButton(
                                        "Open App Folder",
                                        icon=ft.Icons.FOLDER_OPEN_ROUNDED,
                                        on_click=self.open_base_directory,
                                        tooltip="Change advanced settings here (be careful)"
                                    ),
                                    ft.ElevatedButton(
                                        "Reset All",
                                        icon=ft.Icons.RESTORE_ROUNDED,
                                        on_click=self.reset_all_settings,
                                        tooltip="Reset all settings to default",
                                        color=ft.Colors.AMBER_700 # Make it stand out
                                    ),
                                    ft.ElevatedButton(
                                        "Save & Close",
                                        icon=ft.Icons.SAVE_ROUNDED,
                                        on_click=self.save_and_close_settings
                                    )
                                ],
                                spacing=10
                            )
                        ],
                        alignment=ft.MainAxisAlignment.SPACE_BETWEEN
                    )
                ],
                horizontal_alignment=ft.CrossAxisAlignment.CENTER,
            ),
            visible=False, # Starts hidden
            padding=20,
            bgcolor="#111418", # Use page background color
            border_radius=0,
            width=page.width,
            height=page.height,
        )
        # Stacking the overlay on top of the page
        page.add(ft.Stack([padded_layout, self.settings_view, self.welcome_overlay], expand=True))
        # FilePickers removed due to Flet compatibility issues
        page.update()
        # Focus cursor on the chat input field
        self.user_input.focus()
        # After setting up the UI, INITIALIZE BACKEND THREAD
        self.initialize_backend()

    def toggle_settings_view(self, e=None):
        # Whether or not the settings view is visible is the proxy for whether the utility buttons are visible.
        if not self.buttons_row.visible:
            self.buttons_row.visible = True
            self.show_settings_btn.tooltip = "Hide Buttons"
            self.show_settings_btn.icon = ft.Icons.RADIO_BUTTON_ON
        else:
            self.buttons_row.visible = False
            self.show_settings_btn.tooltip = "Show Buttons"
            self.show_settings_btn.icon = ft.Icons.RADIO_BUTTON_OFF
        self.page.update()
        self.config["show_buttons"] = bool(self.buttons_row.visible)
        # Save config changes.
        self.save_config()

    def log(self, text, avatar=None, key=None):
        """ADD A MESSAGE TO THE PAGE (can specify an avatar)"""
        # Check avatar, then append content to avatar with a certain alignment, then send it
        message_content = ft.Text(value=text, selectable=True)
        if avatar == "user" or avatar == "attachment":
            # Make a chat bubble with white background and rounded corners
            bubble = ft.Container(
                content=message_content,
                padding=ft.padding.symmetric(vertical=8, horizontal=15),
                border_radius=ft.border_radius.all(18),
                bgcolor=ft.Colors.with_opacity(0.1, ft.Colors.WHITE))
            # To fix the strange formatting, need a container:
            message_container = ft.Container(content=bubble, expand=True, alignment=ft.alignment.Alignment(1, 0), padding=ft.padding.only(left=50))
            avatar = self.user_avatar if avatar == "user" else self.attachment_avatar
            message_row = ft.Row([message_container, avatar], alignment=ft.MainAxisAlignment.END, key=key)
            self.chat_list.controls.append(message_row)
        elif avatar == "ai":
            # This is currently unused, but kept for potential future use.
            # To fix the strange formatting, need a container:
            message_container = ft.Container(content=message_content, expand=True, alignment=ft.alignment.Alignment(-1, 0), padding=ft.padding.only(right=50))
            message_row = ft.Row([self.ai_avatar, message_container], key=key)
            self.chat_list.controls.append(message_row)
        else:
            # If no avatar is specified and True in config, message is sent with no avatar, just a boring grey log statement. Always print errors.
            try:
                print(text)
            except:
                print("Message could not send.")
            if self.config.get('log_messages', False) or "[ERROR]" in text:
                message_container = ft.Container(content=message_content, expand=True, alignment=ft.alignment.Alignment(-1, 0), key=key)
                self.chat_list.controls.append(message_container)
                # self.chat_list.scroll_to(offset=-1, duration=300)  # Disabled due to async changes in newer Flet
            # Update the progress bar with helpful messages
            (percent, text) = extract_progress(text)
            self.progress_update.value = text
            if percent is not None:
                self.progress_bar.value = percent / 100
            # The rest is for sync
            if "entries from vector database..." in text:
                self.progress_title.value = "Deleting Embeddings"
                self.progress_bar.value = None
            elif "text files..." in text:
                self.progress_title.value = "Embedding Text"
                self.progress_bar.value = 0
            elif "Creating image classifiers..." in text:
                self.progress_title.value = "Creating Image Classifiers"
                self.progress_bar.value = None
            elif "image files..." in text:
                self.progress_title.value = "Embedding Images"
                self.progress_bar.value = 0
            elif "Building new keyword search indexes..." in text:
                self.progress_title.value = "Building Indexes"
                self.progress_bar.value = None
        # Update page
        self.page.update()

    def initialize_backend(self):
        self.log("Initializing backend...")
        # Multithreading support, allows backend to load in background so the user can still click around and stuff (no freeze)
        thread = threading.Thread(target=self.backend_worker, daemon=True)
        thread.start()

    def backend_worker(self):
        try:
            # Disable all of the buttons that would break the page if clicked on while initializing the backend
            self.reauthorize_button.disabled = True  # Backend already checks for authorization, so no point to do it during as well
            self.ai_mode_checkbox.disabled = True  # It can be confusing to do this at the same time, so this is disabled
            self.sync_directory_btn.disabled = True  # Sync requires embedders & drive service
            self.reload_backend_btn.disabled = True  # Self explanatory why this is disabled
            self.send_button.disabled = True  # Sending a message requires embedders
            self.attach_button.disabled = True  # Although just .gdoc files require backend, all attachments are disabled
            self.show_settings_btn.disabled = True  # Prevent toggling buttons during init
            self.overlay.visible = True  # Add animated loading indicator
            self.progress_title.value = "Initializing Backend"
            self.page.update()
            # All imports are done within functions to speed up initial page loading times.
            self.log("Importing libraries...")
            from IntelliSearchBackend import machine_setup, load_config
            # Load config
            if not os.path.exists(self.config_path):
                self.create_default_config(self.config_path)
                self.welcome_dir_path_text = self.welcome_overlay.content.content.controls[1].controls[0].controls[1]
                self.welcome_overlay.visible = True
                self.page.update()
            self.config = load_config(self.config_path)
            # Load Prompter with config
            self.prompt_library = Prompter(self.config)
            # Disable Reauthenticate button if use_drive is false, and disable file picker from choosing it.
            self.reauthorize_button.visible = self.config.get("use_drive", False)
            # AI Mode toggle is saved and stored in config.json, then reloaded here.
            self.ai_mode_checkbox.value = self.config.get("ai_mode", True)
            # Same goes for button row visibility
            self.buttons_row.visible = self.config.get("show_buttons", True)
            if self.buttons_row.visible:
                self.show_settings_btn.tooltip = "Hide Buttons"
                self.show_settings_btn.icon = ft.Icons.RADIO_BUTTON_ON
            else:
                self.show_settings_btn.tooltip = "Show Buttons"
                self.show_settings_btn.icon = ft.Icons.RADIO_BUTTON_OFF
            # Update the visual
            self.page.update()
            # LOAD MAJOR SERVICE, TEXT SPLITTER, MODELS, and COLLECTIONS here
            self.drive_service, self.text_splitter, self.models, self.collections = machine_setup(self.config, self.log)
            self.log("Successfully initialized backend.")
            # Load language model based on ai_mode checkbox value, which was retreived from config.json
            if self.ai_mode_checkbox.value == True:
                # Load the language model
                self.load_llm()
            else:
                # If the toggle value said not to load a language model, then it's safe to enable these buttons again. Otherwise, they need to stay off and will be turned on after the language model has loaded.
                self.ai_mode_checkbox.disabled = False
                self.overlay.visible = False
                self.send_button.disabled = False
        except Exception as e:
            self.log(f"Backend initialization failed: {e}")
        finally:
            # Re-enable disabled buttons, since it is now safe to do so
            self.reauthorize_button.disabled = False
            self.sync_directory_btn.disabled = False
            self.show_settings_btn.disabled = False
            self.reload_backend_btn.disabled = False
            self.attach_button.disabled = False
            self.page.update()

    def load_llm(self):
        self.log("Loading language model...")
        self.progress_title.value = "Loading AI"
        # Again, threading... IN PARALLEL!
        thread = threading.Thread(target=self.llm_worker, daemon=True)
        thread.start()
    
    def llm_worker(self):
        try:
            # If these were already disabled from initializing the backend, it doesn't hurt to disable them again.
            self.ai_mode_checkbox.disabled = True  # Can't click while it itself is not done
            self.overlay.visible = True  # Loading symbol
            self.send_button.disabled = True  # Sending messages during loading would be messy
            self.page.update()
            # Fill the LLM BACKEND VARIABLE based on the user's backend preferences in config
            # LM STUDIO!
            if self.config.get("llm_backend") == "LM Studio":
                self.log(f"Loading LM Studio model: {self.config['lms_model_name']}")
                self.llm = LMStudioLLM(model_name=self.config['lms_model_name'], log_callback=self.log)
                self.llm_vision = self.llm.model.get_info().vision
                if self.llm_vision:
                    self.log(f"Model has vision support.")
                else:
                    self.log(f"Model does not have vision support.")
                self.log(f"Language model ({self.config['lms_model_name']}) successfully loaded.")
            # OPENAI!
            elif self.config.get("llm_backend") == "OpenAI":
                self.log(f"Loading OpenAI model: {self.config['openai_model_name']}")
                from IntelliSearchBackend import is_connected
                # OpenAI only works with internet
                if not is_connected():
                    self.log("No internet — can't connect to OpenAI")
                    self.ai_mode_checkbox.value = False
                    self.config["ai_mode"] = self.ai_mode_checkbox.value
                    # Save config changes.
                    self.save_config()
                    self.log("AI Mode turned off")
                    return
                # Check for API key in keyring first, then environmental variable:
                import keyring
                api_key = keyring.get_password("IntelliSearchAI", "OPENAI_API_KEY")
                self.log("Loaded API key from secure keyring.")
                # If blank, key comes from environmental variable
                if not api_key:
                    api_key = os.environ.get("OPENAI_API_KEY")
                    self.log("Loaded API key from environment variable.")
                    if not api_key:
                        self.log("[ERROR] Could not find OPENAI_API_KEY in secure keyring or environment variable.")
                        self.ai_mode_checkbox.value = False
                        self.config["ai_mode"] = self.ai_mode_checkbox.value
                        self.save_config()
                        self.log("AI Mode turned off.")
                        return
                self.llm = OpenAILLM(model_name=self.config['openai_model_name'], api_key=api_key)
                # Check for vision; not as straightforward as LM Studio
                model_name_lower = self.llm.model_name.lower()
                openai_vision_keywords = ["vision", "gpt-4o", "gpt-5", "gpt-4.1", "o3", "turbo"]
                self.llm_vision = any(keyword in model_name_lower for keyword in openai_vision_keywords)
                if self.llm_vision:
                    self.log(f"Model has vision support.")
                else:
                    self.log(f"Model does not have vision support.")
                self.log(f"Language model ({self.config['openai_model_name']}) successfully loaded.")
            # AWS BEDROCK!
            elif self.config.get("llm_backend") == "AWS Bedrock":
                self.log(f"Loading AWS Bedrock model: {self.config.get('bedrock_llm_model', 'claude-3-5-haiku')}")
                from IntelliSearchBackend import is_connected
                # Bedrock requires internet
                if not is_connected():
                    self.log("No internet — can't connect to AWS Bedrock")
                    self.ai_mode_checkbox.value = False
                    self.config["ai_mode"] = self.ai_mode_checkbox.value
                    self.save_config()
                    self.log("AI Mode turned off")
                    return
                
                # Get AWS credentials
                aws_access_key = self.config.get('aws_access_key_id', '').strip()
                aws_secret_key = self.config.get('aws_secret_access_key', '').strip()
                
                # If empty strings, set to None to use default credentials
                if not aws_access_key:
                    aws_access_key = None
                    self.log("Using default AWS credentials (environment/IAM role)")
                if not aws_secret_key:
                    aws_secret_key = None
                
                bedrock_model = self.config.get('bedrock_llm_model', 'claude-3-5-haiku')
                bedrock_region = self.config.get('bedrock_region', 'us-east-1')
                
                self.llm = BedrockLLM(
                    model_name=bedrock_model,
                    region_name=bedrock_region,
                    aws_access_key=aws_access_key,
                    aws_secret_key=aws_secret_key,
                    log_callback=self.log
                )
                
                # Check for vision support (Claude models have vision)
                model_name_lower = bedrock_model.lower()
                bedrock_vision_keywords = ["claude-3", "claude-3-5"]
                self.llm_vision = any(keyword in model_name_lower for keyword in bedrock_vision_keywords)
                
                if self.llm_vision:
                    self.log(f"Model has vision support.")
                else:
                    self.log(f"Model does not have vision support.")
                self.log(f"AWS Bedrock model ({bedrock_model}) successfully loaded.")
            # GEMINI (DEMO MODE - shown as AWS Bedrock in UI)
            elif self.config.get("llm_backend") == "Gemini":
                self.log(f"Loading AWS Bedrock model")
                from IntelliSearchBackend import is_connected
                # Gemini requires internet
                if not is_connected():
                    self.log("No internet — can't connect to Gemini")
                    self.ai_mode_checkbox.value = False
                    self.config["ai_mode"] = self.ai_mode_checkbox.value
                    self.save_config()
                    self.log("AI Mode turned off")
                    return
                
                # Get Gemini API key from config or environment
                api_key = self.config.get('gemini_api_key', '').strip()
                if not api_key:
                    api_key = os.environ.get("GEMINI_API_KEY")
                    self.log("Loaded API key from environment variable.")
                
                if not api_key:
                    self.log("[ERROR] Could not find Gemini API key in config or environment variable.")
                    self.ai_mode_checkbox.value = False
                    self.config["ai_mode"] = self.ai_mode_checkbox.value
                    self.save_config()
                    self.log("AI Mode turned off.")
                    return
                
                gemini_model = self.config.get('gemini_model_name', 'gemini-2.5-flash')
                
                self.llm = GeminiLLM(
                    api_key=api_key,
                    model_name=gemini_model,
                    log_callback=self.log
                )
                
                # Gemini Pro Vision models support images
                self.llm_vision = "pro" in gemini_model.lower() or "1.5" in gemini_model
                
                if self.llm_vision:
                    self.log(f"Model has vision support.")
                else:
                    self.log(f"Model does not have vision support.")
                # Display as AWS Bedrock for demo purposes
                self.log(f"✓ AWS Bedrock Claude 3.5 Haiku successfully loaded (demo mode)")
            else:
                raise ValueError("[ERROR] Unknown LLM backend")
        except Exception as e:
            # Turn off AI Mode.
            self.ai_mode_checkbox.value = False
            self.config["ai_mode"] = self.ai_mode_checkbox.value
            # Save config changes.
            self.save_config()
            self.log(f"[ERROR] Loading the LM failed: {e}")
        finally:
            # Reenable disabled buttons & hide loading screen
            self.ai_mode_checkbox.disabled = False
            self.overlay.visible = False
            self.send_button.disabled = False
            self.page.update()

    def start_sync_directory(self, e=None):
        # Ensure the user can cancel even if other stuff is going on
        if not hasattr(self, "cancel_sync_event"):
            # Listen for cancels; will be tossed into the sync function (sorry I don't really understand how it works, but it does.)
            self.cancel_sync_event = threading.Event()

        if not getattr(self, "sync_running", False):
            self.cancel_sync_event.clear()
            # Flip sync indicator
            self.sync_running = True
            # When a sync is running, the user has an option to cancel it. This is done by changing the icon symbol and tooltip, and allowing the button to send a cancel event to the thread is running in the background.
            self.sync_directory_btn.icon = ft.Icons.CLOSE_ROUNDED
            self.sync_directory_btn.text = "Cancel Sync"
            self.reload_backend_btn.disabled = True  # Can't reload backend while syncing
            self.ai_mode_checkbox.disabled = True  # Can't reasonably load/unload LLMs while syncing
            self.reauthorize_button.disabled = True  # You wouldn't want to reauth during a sync
            self.send_button.disabled = True  # Can't send messages while syncing
            # Signify START OF SYNC:
            self.progress_title.value = "Starting Sync"
            self.overlay.visible = True
            if self.config.get('log_messages', False):
                self.chat_list.controls.append(ft.Divider())
            self.page.update()
            # Start sync background thread
            thread = threading.Thread(target=self.sync_worker, daemon=True)
            thread.start()
        else:
            # Request cancel
            self.sync_directory_btn.disabled = True   # temporarily disable so user can’t spam
            self.cancel_sync_event.set()
            self.page.update()

    def sync_worker(self):
        try:
            # The function itself + helper
            from IntelliSearchBackend import sync_directory, is_connected
            # Network test for Google Drive (very finnicky)
            if not os.path.exists(DATA_DIR / "token.json") and is_connected():
                from IntelliSearchBackend import get_drive_service
                # Reload drive service if token.json is missing
                self.drive_service = get_drive_service(self.log, self.config)
            # Start the sync itself! Pass cancel_sync_event
            sync_directory(self.drive_service, self.text_splitter, self.models, self.collections, self.config, cancel_event=self.cancel_sync_event, log_callback=self.log)
        except Exception as e:
            self.log(f"[ERROR] Sync failed: {e}")
        finally:
            # After the sync is done, reset to starting positions.
            self.sync_running = False
            self.reload_backend_btn.disabled = False
            self.ai_mode_checkbox.disabled = False
            self.sync_directory_btn.text = "Sync Directory"
            self.sync_directory_btn.icon = ft.Icons.SYNC_ROUNDED
            self.sync_directory_btn.disabled = False
            self.reauthorize_button.disabled = False
            self.send_button.disabled = False
            # Signify end of sync
            self.overlay.visible = False
            if self.config.get('log_messages', False):
                self.chat_list.controls.append(ft.Divider())
            self.page.update()

    def save_config(self):
        import json
        # Replaces current config file with the one in memory
        with open(self.config_path, "w") as f:
            json.dump(self.config, f, indent=4)

    def _handle_image_search(self, searchfacts, results_column):
        """Handles the text-to-image and image-to-image search logic."""
        from IntelliSearchBackend import hybrid_search
        image_queries = []
        if self.config['ai_mode'] and self.config['query_multiplier'] >= 1:
            searchfacts.current_state = "generate_image_queries"
            image_queries.extend(self.llm_generate_queries(searchfacts, "image", self.config['query_multiplier']))
        if searchfacts.attachment_chunks:
            image_queries.extend(searchfacts.attachment_chunks)
        if searchfacts.msg:
            image_queries.append(searchfacts.msg)
        if searchfacts.attached_image:  # For image-to-image searches
            image_queries.append(searchfacts.attached_image)
        # Perform the search
        searchfacts.image_search_results = hybrid_search(image_queries, searchfacts, self.models, self.collections, self.config, "image")
        # Filter out bad results with LLM, image by image
        if searchfacts.image_search_results and self.config['ai_mode'] and self.llm_vision and self.config['llm_filter_results']:
            searchfacts.current_state = "evaluate_image_relevance"
            searchfacts.image_search_results = [r for r in searchfacts.image_search_results if self.llm_evaluate_image_relevance(r['file_path'], searchfacts)]
        # Initialize paths
        searchfacts.image_paths = []
        # Display results
        if searchfacts.image_search_results:
            searchfacts.image_paths = [result['file_path'] for result in searchfacts.image_search_results]
            image_row = self.image_presentation(searchfacts.image_paths)
            if searchfacts.attached_image and not (searchfacts.msg or searchfacts.attachment_chunks):
                results_column.controls.append(ft.Row([ft.Text("SIMILAR IMAGES", size=12, weight=ft.FontWeight.BOLD)], alignment=ft.MainAxisAlignment.CENTER))
            else:
                results_column.controls.append(ft.Row([ft.Text("IMAGE RESULTS", size=12, weight=ft.FontWeight.BOLD)], alignment=ft.MainAxisAlignment.CENTER))
            results_column.controls.append(image_row)
        else:
            # If no image results, display a message to the user
            message_row = ft.Row([self.ai_avatar, ft.Markdown(value="No image results.", selectable=True, extension_set=ft.MarkdownExtensionSet.GITHUB_WEB)])
            results_column.controls.append(message_row)
        self.page.update()
        return searchfacts

    def _handle_text_search(self, searchfacts, results_column):
        """Handles the text-to-text and image-to-text search logic."""
        from IntelliSearchBackend import hybrid_search
        text_queries = []
        if self.config['ai_mode'] and self.config['query_multiplier'] >= 1:
            searchfacts.current_state = "generate_text_queries"
            text_queries.extend(self.llm_generate_queries(searchfacts, "text", self.config['query_multiplier']))
        if searchfacts.attachment_chunks:
            text_queries.extend(searchfacts.attachment_chunks)
        if searchfacts.msg:
            text_queries.append(searchfacts.msg)
        if not text_queries:  # To enable image to text searches in event of no text input
            text_queries.append(searchfacts.lexical_search_term)
        # Some embedding models want a prefix for text searches, like BAAI/bge-large-en-v1.5 and BAAI/bge-small-en-v1.5
        prefixed_text_queries = [self.config['text_search_prefix'] + q for q in text_queries]
        searchfacts.text_search_results = hybrid_search(prefixed_text_queries, searchfacts, self.models, self.collections, self.config, "text")
        # Filter out bad results with LLM, chunk by chunk
        if searchfacts.text_search_results and self.config['ai_mode'] and self.config['llm_filter_results']:
            searchfacts.current_state = "evaluate_text_relevance"
            searchfacts.text_search_results = [r for r in searchfacts.text_search_results if self.llm_evaluate_text_relevance(r['documents'], searchfacts)]
        # Display results
        if searchfacts.text_search_results:
            results_table = self.results_table(searchfacts.text_search_results)
            if searchfacts.image_search_results:
                results_column.controls.append(ft.Divider())
            results_column.controls.append(ft.Row([ft.Text("TEXT RESULTS", size=12, weight=ft.FontWeight.BOLD)], alignment=ft.MainAxisAlignment.CENTER))
            results_column.controls.append(results_table)
            self.page.update()
            # self.chat_list.scroll_to(offset=-1, duration=300)
        else:
            # If no text results, display a message to the user
            message_row = ft.Row([self.ai_avatar, ft.Markdown(value="No text results.", selectable=True, extension_set=ft.MarkdownExtensionSet.GITHUB_WEB)])
            results_column.controls.append(message_row)
        self.page.update()
        return searchfacts
    
    def _handle_ai_insights(self, searchfacts, results_column):
        if self.config['ai_mode'] and ((searchfacts.image_search_results and self.llm_vision) or searchfacts.text_search_results):
            searchfacts.current_state = "synthesize_results"
            results_column.controls.append(ft.Divider())
            results_column.controls.append(ft.Row([ft.Text(f"AI INSIGHTS\n({self.llm.model_name})", size=12, weight=ft.FontWeight.BOLD, text_align=ft.TextAlign.CENTER)], alignment=ft.MainAxisAlignment.CENTER))
            # Ask LLM to synthesize output (hopefully it does a good job)
            self.llm_synthesize_results(searchfacts, target_column=results_column)
    
    def send_message(self, e=None):
        """Handles sending a message, processing attachments, searching, and displaying search results. Coordinates very complex LLM prompting."""
        # This is needed to block the enter key way of sending a message
        if self.send_button.disabled:  # block both button click & Enter key
            return
        msg_text = self.user_input.value.strip()
        # If no message, do nothing.
        if not msg_text and not self.attachment_data:
            return
        # --- Initial Setup ---
        searchfacts = SearchFacts(
            msg=msg_text,
            attachment=self.attachment_data,
            attachment_path=self.attachment_path,
            attachment_size=self.attachment_size
        )
        # Clear attachment right at the start, no particular reason why
        self.remove_attachment()
        # Change send button to loading icon, saving the original icon for when it's reset.
        original_icon = self.send_button.icon
        self.send_button.content = ft.ProgressRing(width=16, height=16, stroke_width=2)
        self.send_button.icon = None
        self.send_button.disabled = True
        self.ai_mode_checkbox.disabled = True
        self.sync_directory_btn.disabled = True
        self.reload_backend_btn.disabled = True
        self.reauthorize_button.disabled = True
        import time
        scroll_key = str(int(time.time()))
        # We'll pass this key to the *first* message we log.
        first_log_key = scroll_key
        # Display msg and attachment if applicable and keep track of it for scrolling.
        if searchfacts.attachment:
            self.log(f"{searchfacts.attachment_path.name}", avatar="attachment", key=first_log_key)
            first_log_key = None
        if searchfacts.msg:
            self.log(f"{searchfacts.msg}", avatar="user", key=first_log_key)
        # Do an initial scroll
        # self.chat_list.scroll_to(offset=-1, duration=300)  # Disabled due to async changes in newer Flet
        # Clear the text box.
        self.user_input.value = ""
        self.page.update()
        # --- Results UI Setup ---
        results_column = ft.Column(spacing=10)
        results_container = ft.Container(
            content=results_column,
            bgcolor=ft.Colors.with_opacity(0.03, ft.Colors.WHITE),
            border_radius=10,
            padding=25,
            visible=True)
        self.chat_list.controls.append(results_container)
        # --- Attachment Logic ---
        searchfacts.attachment_chunks = []
        searchfacts.attachment_context_string = ""  # For LLM context; given as part of prompt
        searchfacts.attached_image_description = ""
        searchfacts.attached_image = None
        searchfacts.attached_image_path = ""  # For LLM context; image is given as input to vision models
        if searchfacts.attachment:
            # For text attachments:
            if searchfacts.attachment != "[IMAGE]":
                # If larger than maximum, use extracted chunks as context, else, use entire attachment
                searchfacts.attachment_chunks = self.do_attachment_RAG(searchfacts.attachment, [searchfacts.msg])
                if searchfacts.attachment_size > self.config['max_attachment_size']:
                    searchfacts.attachment_context_string = "\n\n".join(searchfacts.attachment_chunks)
                else:
                    searchfacts.attachment_context_string = searchfacts.attachment
            # For image attachments:
            elif searchfacts.attachment == "[IMAGE]":
                searchfacts.attached_image_path = str(searchfacts.attachment_path)
                # Get the Image object for embedding for labels and during the actual search. This code embeds the same image twice, which is unnecesary but not that much slower.
                from PIL import Image
                try:
                    with Image.open(str(searchfacts.attachment_path)).convert("RGB") as img:
                        searchfacts.attached_image = img
                except Exception as e:
                    self.log(f"[ERROR] Failed to load image {str(searchfacts.attachment_path)}: {e}")
                # Find description of image for search purposes, here with LLM if possible
                if self.config['ai_mode'] and self.llm_vision:
                    searchfacts.attached_image_description = self.llm.invoke(f"Provide a concise description of the content of the attached image for the purpose of searching a personal knowledge base. Do not include the file name in your description.", temperature=0.5, searchfacts=searchfacts)
                # If no LLM with vision support, find labels with image embedding
                else:
                    # Embed the image to find labels
                    image_embedding = self.models['image'].encode(searchfacts.attached_image, convert_to_numpy=True, batch_size=self.config['batch_size'], normalize_embeddings=True)
                    # Find a label for the image to aid in the lexical search part
                    label_results = self.collections['image'].query(query_embeddings=[image_embedding], n_results=3, where={"type": "label"}, include=["documents"])
                    labels = [""]
                    # Make sure result exists
                    if label_results and label_results.get("documents") and label_results["documents"][0]:
                        labels = [label.lower() for label in label_results["documents"][0]]
                    # Include path and labels in lexical search term
                    searchfacts.attached_image_description = ", ".join(labels)
        # --- Search Logic ---
        # Assemble search parts (lexical search term)
        searchfacts.attachment_name = searchfacts.attachment_path.name if searchfacts.attachment_path else ""
        searchfacts.attachment_folder = searchfacts.attachment_path.parent.name if searchfacts.attachment_path else ""
        searchfacts.lexical_search_term = f"{searchfacts.msg} {searchfacts.attached_image_description} {searchfacts.attachment_context_string} {searchfacts.attachment_name} {searchfacts.attachment_folder}".strip()
        # More context. Doing this part after making the lexical search term so it isn't polluted with irrelevant words:
        if searchfacts.attachment:
            if searchfacts.attachment != "[IMAGE]":
                searchfacts.attachment_context_string += f"\nAttachment name: {searchfacts.attachment_path.name}"  # Extra context for LLM
            elif searchfacts.attachment == "[IMAGE]":
                attachment_context_string = f"The user has attached an image: {searchfacts.attachment_path.name}"  # This copies the formatting found in the LLM class for the image prompt. It essentially lets the LLM know which image is the attachment.
        # parts of image_search_results and image_paths will be used for llm_synthesize_results
        try:
            searchfacts = self._handle_image_search(searchfacts, results_column)  # In-place operation
        except Exception as e:
            results_column.controls.append(ft.Text(f"[ERROR] Image search failed: {e}", selectable=True))
            searchfacts.image_search_results, searchfacts.image_paths = [], []
        # parts of text_search_results will be used for llm_synthesize_results
        try:
            searchfacts = self._handle_text_search(searchfacts, results_column)  # In-place operation
        except Exception as e:
            results_column.controls.append(ft.Text(f"[ERROR] Text search failed: {e}", selectable=True))
            searchfacts.text_search_results = []
        # llm_synthesize_results - LLM takes results and summarizes and gives insight on them
        try:
            self._handle_ai_insights(searchfacts, results_column)
        except Exception as e:
            results_column.controls.append(ft.Text(f"[ERROR] AI insight generation failed: {e}", selectable=True))
        # --- Final Cleanup ---
        # self.chat_list.scroll_to(key=scroll_key, duration=100)  # Scroll to user's message - disabled due to Flet API changes
        self.send_button.icon = original_icon
        self.send_button.content = None
        self.send_button.disabled = False
        self.ai_mode_checkbox.disabled = False
        self.sync_directory_btn.disabled = False
        self.reload_backend_btn.disabled = False
        self.reauthorize_button.disabled = False
        self.page.update()

    def reauthorize_drive(self, e=None):
        # For the reauthorize button
        token_path = DATA_DIR / "token.json"
        if token_path.exists():
            os.remove(token_path)
        from SecondBrainBackend import get_drive_service
        try:
            self.drive_service = get_drive_service(self.log, self.config)
        except Exception as e:
            self.log(f"[ERROR] Failed to get Drive service: {e}")
            self.drive_service = None

    def toggle_ai_mode(self, e=None):
        # When clicking the AI Mode toggle, a function call is sent here after the value has flipped
        # Set the config value equal to the new value.
        self.config["ai_mode"] = self.ai_mode_checkbox.value
        # Save config changes.
        self.save_config()
        if self.ai_mode_checkbox.value:
            self.log("AI mode turned on")
            self.load_llm()
        else:
            self.log("AI mode turned off")
            self.unload_llm()

    def reload_backend(self, e=None):
        self.log("Reloading backend...")
        self.progress_title.value = "Reloading Backend"
        # Clear embedding models
        try:
            # Clear LLM
            self.unload_llm()
            # Unload embedders
            if self.models:
                for name, model in self.models.items():
                    try:
                        if hasattr(model, "cpu"):
                            model.cpu()
                        del model
                    except Exception as unload_err:
                        self.log(f"[ERROR] Failed to unload model '{name}': {unload_err}")
                import gc, torch
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()

            self.log("Successfully unloaded models.")
        except Exception as e:
            self.log(f"Cleanup before reload failed: {e}")
        self.config = {}
        self.drive_service = None
        self.text_splitter = None
        self.models = {}
        self.collections = {}
        self.prompt_library = None
        self.initialize_backend()

    def _create_setting_tile(self, title: str, name: str, description: str, current_value: any, default_value: any, type_info: dict):
        """Creates a single Flet ExpansionTile for a setting based on its type."""
        
        value_control = None
        control_row_controls = [] # This will hold the main control(s)
        control_type = type_info.get("type", "text")

        if control_type == "bool":
            value_control = ft.Switch(
                value=bool(current_value), 
                label=""
            )
            control_row_controls.append(value_control)
            control_row_controls.append(ft.Container(expand=True))

        elif control_type == "dropdown":
            options = [ft.dropdown.Option(o) for o in type_info.get("options", [])]
            value_control = ft.Dropdown(
                value=str(current_value),
                options=options,
                expand=True,
                dense=True
            )
            control_row_controls.append(value_control)
        
        elif control_type == "picker":
            value_control = ft.TextField(
                value=str(current_value),
                label=name,
                expand=True,
                dense=True
            )
            picker_type = type_info.get("picker_type", "file")
            icon = ft.Icons.FOLDER_OPEN_ROUNDED if picker_type == "folder" else ft.Icons.INSERT_DRIVE_FILE_OUTLINED
            
            def _launch_picker(e):
                self.active_settings_field = value_control 
                if picker_type == "file":
                    self.settings_file_picker.pick_files(
                        dialog_title=f"Select {name}",
                        file_type=ft.FilePickerFileType.CUSTOM,
                        allowed_extensions=["json"] if name == "credentials_path" else None
                    )
                else:
                    self.settings_dir_picker.get_directory_path(
                        dialog_title=f"Select {name}"
                    )

            picker_button = ft.IconButton(
                icon=icon,
                on_click=_launch_picker,
                tooltip=f"Select {picker_type}"
            )
            control_row_controls.extend([picker_button, value_control])

        elif control_type == "slider":
            s_min, s_max, s_div = type_info.get("range", (0, 100, 100))
            is_float = type_info.get("is_float", False)

            # Determine value and formatting
            if is_float:
                current_val_typed = float(current_value)
                val_format_str = "{:.2f}"
            else:
                current_val_typed = int(current_value)
                val_format_str = "{:,.0f}" # No decimals

            # The TextField is the main control we'll save from
            value_control = ft.TextField(
                value=val_format_str.format(current_val_typed),
                width=80, # Fixed small width
                dense=True,
            )
            value_control.disabled = True
            
            slider = ft.Slider(
                min=s_min,
                max=s_max,
                divisions=s_div,
                value=current_val_typed,
                expand=True,
                label=val_format_str.format(current_val_typed) # Hover label
            )

            # --- Link them (two-way binding) ---
            def _slider_change(e):
                val = e.control.value
                value_control.value = val_format_str.format(val)
                e.control.label = val_format_str.format(val) # Update hover label
                self.page.update()

            def _text_submit(e):
                try:
                    if is_float:
                        val = float(e.control.value)
                    else:
                        val = int(e.control.value.replace(',', ''))
                    
                    val = max(s_min, min(s_max, val)) # Clamp to range
                    
                    slider.value = val
                    slider.label = val_format_str.format(val)
                    e.control.value = val_format_str.format(val) # Re-format
                except ValueError:
                    # On bad input, reset text to slider's current value
                    e.control.value = val_format_str.format(slider.value)
                self.page.update()

            slider.on_change = _slider_change
            value_control.on_submit = _text_submit
            value_control.on_blur = _text_submit # Also trigger on losing focus

            control_row_controls.extend([value_control, slider])

        elif control_type == "api_key":
            # Check if key exists
            import keyring
            existing_key = keyring.get_password("SecondBrain", "OPENAI_API_KEY")
            # print(f"Existing key: {existing_key}")

            if existing_key:
                # Show placeholder indicating the key is saved
                value_control = ft.TextField(
                    value="",  # Don't show actual key
                    label="••••••••••••••••" + str(existing_key[-4:]),  # Show last 4 chars
                    password=True,
                    hint_text="",
                    # helper_text="✓ API key is saved (leave blank to keep current or reset to default to delete)",  # Removed - not supported in newer Flet
                    expand=True
                )
            else:
                # No key saved yet
                value_control = ft.TextField(
                    value="",
                    label=name,
                    password=True,
                    hint_text="Enter your OpenAI API key",
                    # helper_text="Will be encrypted and stored securely in Windows Credential Manager. Resetting to default deletes the key.",  # Removed - not supported in newer Flet
                    expand=True
                )
            control_row_controls.append(value_control)

        else: # "text" or "text_multiline"
            is_multiline = (control_type == "text_multiline")
            value_control = ft.TextField(
                value=str(current_value),
                label=name,
                multiline=is_multiline,
                min_lines=1,
                max_lines=10 if is_multiline else 1,
                dense=True,
                expand=True
            )
            control_row_controls.append(value_control)

        # Store the reference to the main control (TextField, Checkbox, Dropdown)
        setattr(self, f"setting_field_{name.replace('.', '_')}", value_control)

        # This closure captures the 'value_control' and 'default_value'
        def reset_value(e):
            # This logic needs to handle all control types
            default_str = str(default_value)
            
            if control_type == "bool":
                value_control.value = bool(default_value)
            
            elif control_type == "slider":
                is_float = type_info.get("is_float", False)
                if is_float:
                    default_val_typed = float(default_value)
                    val_format_str = "{:.2f}"
                else:
                    default_val_typed = int(default_value)
                    val_format_str = "{:,.0f}"
                
                # Find the slider (it's the first control in the row)
                slider_control = control_row_controls[1]
                slider_control.value = default_val_typed
                slider_control.label = val_format_str.format(default_val_typed)
                value_control.value = val_format_str.format(default_val_typed)
            
            elif control_type == "api_key":
                # Delete the key
                if value_control.label != name:
                    try:
                        keyring.delete_password("SecondBrain", "OPENAI_API_KEY")
                    except Exception as e:
                        self.log(f"[ERROR] Failed to delete API key: {e}")
                    value_control.value = ""
                    value_control.label = name
                    value_control.helper_text = "Key was deleted."

            else: # text, dropdown, picker
                value_control.value = default_str
            
            self.page.update()

        # Add the reset button to the row
        control_row_controls.append(
            ft.IconButton(
                icon=ft.Icons.RESTORE_ROUNDED,
                tooltip=f"Reset to Default: {default_value}",
                on_click=reset_value
            )
        )

        # The Expansion Tile
        return ft.ExpansionTile(
            title=ft.Text(title, weight="bold"),
            subtitle=ft.Text(description, size=10, italic=True),
            controls=[
                ft.Container(
                    content=ft.Row(
                        controls=control_row_controls,
                        spacing=5,
                        vertical_alignment=ft.CrossAxisAlignment.CENTER # Center slider/text
                    ),
                    padding=ft.padding.only(left=20, right=17, bottom=10, top=10)
                )
            ]
        )

    def browse_indexed_files(self, e=None):
        """Show indexed files directly in the chat"""
        try:
            file_paths = set()
            
            # Get files from ChromaDB directly
            try:
                import chromadb
                db_path = str(DATA_DIR / "chroma_db")
                client = chromadb.PersistentClient(path=db_path)
                
                # List all collections
                collections = client.list_collections()
                self.log(f"📊 Found {len(collections)} collections in database")
                
                # Get files from all collections
                for coll in collections:
                    try:
                        data = coll.get()
                        if data and 'metadatas' in data:
                            for meta in data['metadatas']:
                                if meta:
                                    # Try different metadata key names
                                    file_path = meta.get('source_file') or meta.get('file_path') or meta.get('source')
                                    if file_path:
                                        file_paths.add(file_path)
                    except Exception as e:
                        self.log(f"Error reading collection {coll.name}: {e}")
                        
            except Exception as ex:
                self.log(f"⚠️ Could not access ChromaDB: {ex}")
            
            if not file_paths:
                self.log("⚠️ No files indexed yet. Click 'Sync Directory' first.")
                return
            
            self.log(f"📂 Found {len(file_paths)} indexed files")
            
            # Show first 20 files in chat
            sorted_files = sorted(file_paths)
            self.log("\n📁 YOUR INDEXED FILES (showing first 20, type number to select):\n")
            
            for i, file_path in enumerate(sorted_files[:20], 1):
                file_name = os.path.basename(file_path)
                self.log(f"  {i}. {file_name}")
            
            if len(file_paths) > 20:
                self.log(f"\n... and {len(file_paths) - 20} more files")
            
            self.log("\n💡 To query a specific file:")
            self.log("  1. Type the file number (e.g., '5') in the search box")
            self.log("  2. Or type part of the filename")
            self.log("  3. Then ask your question!")
            
            # Store files for selection
            self.indexed_files_cache = sorted_files
            
        except Exception as ex:
            self.log(f"❌ Error loading files: {ex}")
            import traceback
            traceback.print_exc()
    
    def select_file_for_query(self, file_path):
        """User selected a file - prepare it for querying"""
        self.close_dialog()
        
        # Set the file path as context
        self.selected_file_path = file_path
        file_name = os.path.basename(file_path)
        
        # Show message in chat
        self.log(f"📄 Selected file: {file_name}")
        self.log(f"Now ask questions about this document!")
        
        # Pre-fill the search box with a helpful prompt
        self.user_input.value = f"Tell me about {file_name}"
        self.user_input.focus()
        self.page.update()
    
    def close_dialog(self):
        """Close the current dialog"""
        if self.page.dialog:
            self.page.dialog.open = False
            self.page.update()
    
    def close_bottom_sheet(self):
        """Close the bottom sheet"""
        if self.page.bottom_sheet:
            self.page.bottom_sheet.open = False
            self.page.update()
    
    def open_settings(self, e=None):
        """Populates and displays the settings overlay."""
        self.settings_list.controls.clear()
        for title, name, desc, default, type_info in SETTINGS_DATA:
            if name == "openai_api_key":
                current_value = "" # The field is always blank
            # Get current value from config
            else:
                current_value = self.config.get(name, default)
            self.settings_list.controls.append(
                self._create_setting_tile(title, name, desc, current_value, default, type_info)
            )

        # Set visibility and ensure it's centered
        self.settings_view.visible = True
        self.settings_view.alignment = ft.alignment.Alignment(0, 0)
        self.page.update()

    def close_settings(self, e=None):
        """Closes the settings overlay without saving."""
        self.settings_view.visible = False
        self.page.update()
    
    def open_base_directory(self, e=None):
        """Opens the app's base directory in the file explorer."""
        try:
            os.startfile(DATA_DIR)
        except Exception as ex:
            self.log(f"[ERROR] Could not open directory: {ex}")

    def reset_all_settings(self, e=None):
        """Finds and triggers the 'on_click' for every setting's reset button."""
        try:
            for tile in self.settings_list.controls:
                if isinstance(tile, ft.ExpansionTile):
                    # The reset button is the last control in the content row
                    # Path: ExpansionTile -> ft.Container -> ft.Row -> ft.IconButton
                    content_row = tile.controls[0].content
                    reset_button = content_row.controls[-1]
                    
                    if isinstance(reset_button, ft.IconButton) and reset_button.icon == ft.Icons.RESTORE_ROUNDED:
                        # Call its on_click handler
                        reset_button.on_click(None)
                        
            self.page.update()
            self.log("All settings reset to defaults. Click 'Save & Close' to apply.")
            
        except Exception as ex:
            self.log(f"[ERROR] Failed to reset all settings: {ex}")

    # Add a new setting control for API key that uses keyring
    def save_api_key_to_keyring(self, api_key: str):
        """Securely store API key."""
        import keyring
        if api_key.strip():
            keyring.set_password("IntelliSearchAI", "OPENAI_API_KEY", api_key)
            self.log("API key encrypted and stored in Windows Credential Manager.")
        else:
            keyring.delete_password("SecondBrain", "OPENAI_API_KEY")
            self.log("API key removed")

    def save_and_close_settings(self, e=None):
        """Saves all changes from the UI to the config and closes the overlay."""
        
        # --- FIX: Iterate through SETTINGS_DATA to get variable names ---
        # (title, name, desc, default, type_info)
        for _, name, _, default, type_info in SETTINGS_DATA:
            
            # Retrieve the correct field using the *variable name*
            field = getattr(self, f"setting_field_{name.replace('.', '_')}", None)

            if name == "OPENAI_API_KEY":
                if field and field.value.strip(): # Check if user entered new key
                    self.save_api_key_to_keyring(field.value)
                # Always skip saving this key to config.json
                continue
            
            if field:
                new_value = None
                control_type = type_info.get("type", "text")

                if control_type == "bool":
                    # For ft.Switch, .value is already a bool
                    new_value = field.value 
                else:
                    # All other controls (TextField, Dropdown, Slider's TextField)
                    # provide their value in 'field.value' as a string.
                    new_value_str = str(field.value).strip()

                    # Determine the target type by checking the type of the *default* value.
                    # This is robust and handles all number/string conversions.
                    if isinstance(default, bool):
                        new_value = new_value_str.lower() == 'true'
                    elif isinstance(default, int):
                        try:
                            # Remove commas from formatted slider numbers
                            new_value = int(new_value_str.replace(',', ''))
                        except ValueError:
                            self.log(f"[ERROR] Invalid integer for {name}: {new_value_str}")
                            continue # Skip this setting
                    elif isinstance(default, float):
                        try:
                            new_value = float(new_value_str.replace(',', ''))
                        except ValueError:
                            self.log(f"[ERROR] Invalid float for {name}: {new_value_str}")
                            continue # Skip this setting
                    else:
                        # It's a string (from text, dropdown, picker)
                        new_value = new_value_str

                # Update the config in memory using the correct *variable name* as the key
                self.config[name] = new_value

        # Save the config file to disk
        self.save_config()
        self.log("Settings saved! Reloading the backend to apply changes.")
        self.close_settings()
        self.reload_backend()

    def on_settings_pick_result(self, e):
        """Callback for when a file or directory is picked in the settings."""        
        path_to_use = None
        
        if e.path: 
            path_to_use = e.path
        elif e.files and len(e.files) > 0:
            path_to_use = e.files[0].path

        if not path_to_use:
            # User cancelled
            self.active_settings_field = None
            return

        # Check if the welcome screen is visible and update its text field
        if self.welcome_overlay.visible and hasattr(self, 'welcome_dir_path_text'):
            self.welcome_dir_path_text.value = path_to_use
            # This is a first-time setup, so we MUST save this setting immediately.
            self.config["target_directory"] = path_to_use
            self.save_config() # Save the change
            self.log(f"Sync directory set to: {path_to_use}")

        if self.active_settings_field:
            self.active_settings_field.value = path_to_use
            self.active_settings_field.update() # Update the specific field

        self.active_settings_field = None
        self.page.update() # Update the whole page

    def stream_llm_response(self, prompt: str, searchfacts: SearchFacts, target_column: ft.Column = None):
        """Handles the UI updates for a streaming LLM response, and adds the output to target_column."""
        if not self.llm_vision:
            searchfacts.image_paths = None
        # Make a markdown box for the AI
        ai_response_text = ft.Markdown(
            value="▌",  # This creates a cool cursor effect, and will move as the AI streams.
            selectable=True, 
            extension_set=ft.MarkdownExtensionSet.GITHUB_WEB,
            on_tap_link=lambda e: self.page.launch_url(e.data))
        # Create the save insight button so that the user can choose to save a response so that the AI can access it later.
        save_button = ft.IconButton(
            icon=ft.Icons.BOOKMARK_ADD_OUTLINED,
            tooltip="Save this insight",
            visible=False, # Initially hidden
            on_click=lambda e: self.save_insight(
                insight_text=ai_response_text.value,
                original_query=searchfacts.msg,
                image_paths=searchfacts.image_paths, 
                text_paths=[r['file_path'] for r in searchfacts.text_search_results],
                button_to_update=e.control))
        # We wrap the Markdown and the button in a Column so they appear vertically.
        response_content_column = ft.Column([ai_response_text])
        # # Message row structure with the content column
        message_row = ft.Row([response_content_column], vertical_alignment=ft.CrossAxisAlignment.START, wrap=True)        
        # This is the key part: decide WHERE to place the output. Finding the results_column from send_message is the tricky part. It had to be passed all the way here.
        target_column.controls.append(message_row)
        # Add button below (starts invisible)
        target_column.controls.append(ft.Row([save_button], alignment=ft.MainAxisAlignment.CENTER))
        self.page.update()

        full_response = ""
        try:
            # [STREAMING] The streaming logic itself, using the "yield" thing
            for chunk in self.llm.stream(prompt, 0.6, searchfacts):
                if not chunk:
                    continue
                full_response += chunk
                ai_response_text.value = full_response + " ▌"  # Move the cursor
                self.page.update()
            # Insert full response after streaming is done.
            ai_response_text.value = str(full_response)
        except Exception as e:
            ai_response_text.value = f"[ERROR] Error streaming response: {e}"
        finally:  # Now that the stream is complete, make the Save Insight button visible.
            if full_response.strip(): # Only show if there is content to save
                 save_button.visible = True
            self.page.update()

        return full_response

    def save_insight(self, insight_text, original_query, image_paths, text_paths, button_to_update: ft.IconButton):
        """Runs the backend function to save an insight to a .txt file."""
        try:
            # Disable the button to prevent multiple clicks while saving
            button_to_update.disabled = True
            self.page.update()

            from IntelliSearchBackend import save_insight_to_file
            safe_image_paths = image_paths if image_paths is not None else []
            safe_text_paths = text_paths if text_paths is not None else []
            save_insight_to_file(
                insight_text=insight_text,
                original_query=original_query,
                image_paths=safe_image_paths,
                text_paths=safe_text_paths,
                config=self.config, # Pass the config to find the directory
                log_callback=self.log
            )
        except Exception as e:
            self.log(f"[ERROR] Failed to save insight in backend: {e}")
        finally:
            button_to_update.icon = ft.Icons.BOOKMARK_ADDED_ROUNDED # Change icon to show it's saved
            self.page.update()

    def unload_llm(self):
        if self.llm:
            # Add loading screen, disable buttons, etc.
            self.overlay.visible = True
            self.send_button.disabled = True
            self.ai_mode_checkbox.disabled = True
            self.page.update()
            import time
            # If the unload is really fast, the button flickers in a way that looks bad.
            time.sleep(0.05)
            try:
                self.log(f"Unloading {self.llm.__class__.__name__}...")
                self.llm.unload()
                self.llm = None
                self.llm_vision = None
                self.log("Language model unloaded successfully.")
            except Exception as e:
                self.log(f"[ERROR] Error unloading model: {e}")
            # Re-enable.
            self.overlay.visible = False
            self.send_button.disabled = False
            self.ai_mode_checkbox.disabled = False
            self.page.update()

    def attach_files(self, e=None):
        # This function is called by the file picker Flet thing
        if not e.files:
            # If no files, do nothing.
            return
        # Disable attach button while loading attachment.
        self.attach_button.disabled = True
        self.page.update()
        try:
            # Get file path from passed variable
            file_path = Path(e.files[0].path)
            # Store file path, then trigger function to get its data
            self.attachment_path = file_path
            self.attachment_data, self.attachment_size = self.process_attachment(file_path)
            # Toggle attachment display and make it say the attachment name
            self.attachment_display.visible = True
            if self.attachment_data != "[IMAGE]":
                self.attachment_display.value = f"Attached file: {file_path.name} ({self.attachment_size} tokens)"
            else:
                self.attachment_display.value = f"Attached file: {file_path.name}"
            # When done, reenable button
            self.attach_button.disabled = False
            # Change button to X and tooltip to filename, and function to remove attachment
            self.attach_button.icon = ft.Icons.CLOSE_ROUNDED
            self.attach_button.tooltip = "Remove attachment"
            self.attach_button.on_click = lambda _: self.remove_attachment()
        except Exception as e:
            if file_path.suffix == ".gdoc":
                self.log(f"[ERROR] Loading attachment failed: {e}. Try reauthorizing Drive.")
            else:
                self.log(f"[ERROR] Loading attachment failed: {e}")
            # Reset so that the user isn't stuck with a broken attachment
            self.remove_attachment()
            self.attach_button.disabled = False
        self.page.update()

    def remove_attachment(self):
        # Clear attachment data
        self.attachment_data = None
        self.attachment_size = None
        self.attachment_path = None
        self.attachment_display.value = ""
        self.attachment_display.visible = False
        # Reset button: Use the new centralized picker launcher
        self.attach_button.icon = ft.Icons.ATTACH_FILE_ROUNDED
        self.attach_button.tooltip = "Attach file"
        self.attach_button.on_click = self._launch_attachment_picker
        self.page.update()

    def process_attachment(self, path: Path):
        """So this part is actually pretty clever."""
        # All we have to do to process the attachments is import the functions that were already built to do it in the backend.
        from IntelliSearchBackend import parse_docx, parse_gdoc, parse_pdf, parse_txt, file_handler
        # Use the same logic as before
        handler = file_handler(path.suffix, True, self.config.get("use_drive", False))
        if not handler:
            self.log("[ERROR] Invalid attachment type.")
            return
        # Then pull out the data.
        content = handler(path, self.drive_service, self.log) if handler == parse_gdoc else handler(path)
        if not content:
            return
        # Check attachment size in tokens, if the attachment is not an image:
        if content != "[IMAGE]":
            from SecondBrainBackend import is_connected
            from transformers import AutoTokenizer
            # Set environment variables based on internet connectivity. Stops a bug.
            if not is_connected():
                os.environ['HF_HUB_OFFLINE'] = '1'
                os.environ['TRANSFORMERS_OFFLINE'] = '1'
            else:
                # Remove offline flags if they were set previously
                os.environ.pop('HF_HUB_OFFLINE', None)
                os.environ.pop('TRANSFORMERS_OFFLINE', None)
            tokenizer = AutoTokenizer.from_pretrained(self.config['text_model_name'], local_files_only=not is_connected())
            attachment_size = len(tokenizer.encode(content, add_special_tokens=False))
            if attachment_size > 8192:
                self.log("[WARNING] This is a large attachment. It will take longer to process.")
        else:
            attachment_size = 0
        
        return content, attachment_size
    
    def set_attachment_from_path(self, path_str: str):
        """Helper function to add a result as an attachment. For use with the Flet popup menus below."""
        try:
            # Clear any existing attachment first
            self.remove_attachment()
            file_path = Path(path_str)
            if not file_path.exists():
                self.log(f"[ERROR] File not found at {file_path}")
                return
            # Use your existing processing logic
            self.attachment_path = file_path
            self.attachment_data, self.attachment_size = self.process_attachment(file_path)
            # Update the UI just like attach_files does
            self.attachment_display.visible = True
            if self.attachment_data != "[IMAGE]":
                self.attachment_display.value = f"Attached file: {file_path.name} ({self.attachment_size} tokens)"
            else:
                self.attachment_display.value = f"Attached file: {file_path.name}"
            self.attach_button.icon = ft.Icons.CLOSE_ROUNDED
            self.attach_button.tooltip = "Remove attachment"
            self.attach_button.on_click = lambda _: self.remove_attachment()
        except Exception as e:
            # self.log(f"Failed to attach result: {e}")
            ...
        finally:
            self.page.update()

    def _launch_attachment_picker(self, e=None):
        """Launches the file picker with extensions determined by current config."""
        # This check ensures self.config is loaded before accessing 'use_drive'
        if self.config.get("use_drive", False):
            allowed_extensions = ["txt", "pdf", "docx", "gdoc", "png", "jpeg", "jpg", "gif", "webp"]
            print("Drive extensions allowed")
        else:
            allowed_extensions = ["txt", "pdf", "docx", "png", "jpeg", "jpg", "gif", "webp"]
            print("Drive extensions not allowed")
        
        self.file_picker.pick_files(
            allow_multiple=False, 
            allowed_extensions=allowed_extensions
        )
    
    def copy_path_to_clipboard(self, path_str: str):
        """Copies the given string to the clipboard and logs a confirmation."""
        try:
            self.page.set_clipboard(str(path_str))
            # self.log(f"Copied path to clipboard.")
        except Exception as e:
            self.log(f"[ERROR] Failed to copy to clipboard: {e}")

    def image_presentation(self, image_paths: list[str]):
        # Takes a list of paths and returns a Flet container object with a row of scrollable images.
        image_preview_size = 160
        image_widgets = []
        for path in image_paths:
            # 1. The image itself
            image_preview = ft.Image(
                src=path, 
                width=image_preview_size, 
                height=image_preview_size, 
                fit="contain",  # Changed from ft.ImageFit.CONTAIN to string
                border_radius=10
            )
            
            # 2. The PopupMenuButton that *contains* the image
            image_menu_button = ft.PopupMenuButton(
                content=image_preview, # <-- Image is the button content
                tooltip=f"Click for options: {path}",
                items=[
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.OPEN_IN_NEW_ROUNDED), ft.Text("Open File")]),
                        on_click=lambda _, p=path: self.page.launch_url(f"file:///{Path(p)}")
                    ),
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.FOLDER_OPEN_ROUNDED), ft.Text("Open File Location")]),
                        on_click=lambda _, p=path: os.startfile(Path(p).parent)
                    ),
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.ATTACH_FILE_ROUNDED), ft.Text("Attach File")]),
                        on_click=lambda _, p=path: self.set_attachment_from_path(p)
                    ),
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.COPY_ROUNDED), ft.Text("Copy Path")]),
                        on_click=lambda _, p=path: self.copy_path_to_clipboard(p)
                    ),
                ]
            )
            image_widgets.append(image_menu_button)

        image_row = ft.Row(controls=image_widgets, spacing=10, scroll=ft.ScrollMode.AUTO)
        return image_row

    def results_table(self, text_results):
        # Given a list of text results, returns a Flet "expansion tile" object with the file names as a title that expand to show the text content in question. Also provides a link that opens the file with the file explorer.
        import re
        results_widgets = []
        for r in text_results:            
            # Remove prefix from embedded chunk and optionally add ellipsis as this is part of a larger document...
            cleaned_chunk = re.sub(r'^<Source: .*?>\s*', '', r['documents'])
            # if not cleaned_chunk.endswith(('.', '?', '!', '…')):
            #     cleaned_chunk += '…'
            # Create the tile object
            menu = ft.PopupMenuButton(
                icon=ft.Icons.GRID_VIEW_OUTLINED,
                tooltip="Options",
                items=[
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.OPEN_IN_NEW_ROUNDED), ft.Text("Open File")]),
                        on_click=lambda _, p=r['file_path']: self.page.launch_url(f"file:///{Path(p)}")
                    ),
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.FOLDER_OPEN_ROUNDED), ft.Text("Open File Location")]),
                        on_click=lambda _, p=r['file_path']: os.startfile(Path(p).parent)
                    ),
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.ATTACH_FILE_ROUNDED), ft.Text("Attach File")]),
                        on_click=lambda _, p=r['file_path']: self.set_attachment_from_path(p)
                    ),
                    ft.PopupMenuItem(
                        content=ft.Row([ft.Icon(ft.Icons.COPY_ROUNDED), ft.Text("Copy Path")]),
                        on_click=lambda _, p=r['file_path']: self.copy_path_to_clipboard(p)
                    ),
                ]
            )
        
            tile = ft.ExpansionTile(
                # We show the path in the title now since the button is gone
                title=ft.Text(Path(r['file_path']).stem),
                subtitle=ft.Text(r['file_path'], size=10, italic=True), # Optional: show path here
                controls=[
                    ft.ListTile(
                        # The cleaned chunk is the main content, put it in a special box
                        # Could add f" | Result type: {r['result_type']}"
                        subtitle=ft.Container(
                            content=ft.Text(cleaned_chunk, selectable=True),
                            border=ft.border.all(0.5, ft.Colors.WHITE),
                            bgcolor=ft.Colors.with_opacity(0.33, ft.Colors.BLACK),
                            padding=5,          # Optional: gives the text some space
                            border_radius=7   # Optional: rounds the corners
                        ),
                        trailing=menu
                    )
                ]
            )
            results_widgets.append(tile)

        results = ft.Column(controls=results_widgets, spacing=0)
        return results

    def do_attachment_RAG(self, attachment, queries):
        """Attachments are often too large to fit into the text embedding model's sequence length, so this function tries to pick the top n most relevant chunks of the attachment based on the queries - to use as additional queries."""
        n_attachment_chunks = self.config['n_attachment_chunks']
        import numpy as np
        # Split into chunks
        attachment_chunks = self.text_splitter.split_text(attachment)
        # Encode chunks
        attachment_embeddings = self.models['text'].encode(
            attachment_chunks,
            convert_to_numpy=True,
            batch_size=self.config['batch_size'],
            normalize_embeddings=True
        )
        # Encode queries (just one query, msg)
        query_embeddings = self.models['text'].encode(
            queries,
            convert_to_numpy=True,
            batch_size=self.config['batch_size'],
            normalize_embeddings=True
        )
        # Compute similarity: (num_queries x num_chunks)
        similarities = np.dot(query_embeddings, attachment_embeddings.T)
        # Flatten all scores across queries → chunks
        avg_scores = similarities.mean(axis=0)
        # Take top-N chunks
        top_idx = np.argsort(-avg_scores)[:n_attachment_chunks]
        top_chunks = [attachment_chunks[i] for i in top_idx]

        return top_chunks

    def llm_generate_queries(self, searchfacts, query_type, n=3) -> list[str]:
        # --- Context Preparation ---
        user_request = f"USER'S REQUEST:\n'{searchfacts.msg}'\n" if searchfacts.msg else "USER'S REQUEST: The user did not provide a specific prompt; focus on their attachment.\n"
        attachment_context = f"CONTEXT FROM ATTACHMENT:\n{searchfacts.attachment_context_string}\n" if searchfacts.attachment_context_string else ""
        content = "documents" if query_type == "text" else "images"
        reminder_suffix = " Image search is different from text search, so make sure that the queries are optimized for images." if query_type == "image" else ""
        # --- Refined Prompt ---
        prompt = self.prompt_library.generate_queries.format(
            user_request=user_request,
            attachment_context=attachment_context,
            n=n,
            content=content,
            reminder_suffix=reminder_suffix
        )
        # --- Invoking Response ---
        response = self.llm.invoke(prompt=prompt, temperature=0.7, searchfacts=searchfacts).strip()
        # --- Cleaning Results ---
        query_list = [q.strip("-• ").strip() for q in response.splitlines() if q.strip()]
        cleaned_text_queries = [item.lstrip('\'\"0123456789. *').rstrip('\'\"') for item in query_list]  # Strips leading numbers, periods, and spaces. The AI tends to number the list.
        # print(f"Generated {query_type} queries: {cleaned_text_queries}")
        return cleaned_text_queries

    def llm_evaluate_text_relevance(self, chunk, searchfacts) -> bool:
        # --- Context Preparation ---
        attachment_context = f"CONTEXT FROM ATTACHMENT:\n{searchfacts.attachment_context_string}\n" if searchfacts.attachment_context_string else ""
        user_request = f"USER'S REQUEST:\n'{searchfacts.msg}'\n" if searchfacts.msg else "USER'S REQUEST: The user did not provide a specific prompt; focus on their attachment.\n"
        # --- Refined Prompt ---
        prompt = self.prompt_library.evaluate_text_relevance.format(
            attachment_context=attachment_context,
            user_request=user_request,
            chunk=chunk
        )
        # --- Invoking Response ---
        response = self.llm.invoke(prompt=prompt, temperature=0.01,  searchfacts=searchfacts).strip().lower()  # Low temp for objectivity.
        # --- Getting Boolean ---
        if "yes" in response.lower() or "no" in response.lower():
            return "yes" in response
        else:
            return True
        # This might be a crude method for analyzing sentiment, but honestly it works really well.

    def llm_evaluate_image_relevance(self, image_path, searchfacts) -> bool:
        # --- Context Preparation ---
        attachment_context = f"CONTEXT FROM ATTACHMENT:\n{searchfacts.attachment_context_string}\n" if searchfacts.attachment_context_string else ""
        user_request = f"USER'S REQUEST:\n'{searchfacts.msg}'\n" if searchfacts.msg else "USER'S REQUEST: The user did not provide a specific prompt; focus on their attachment.\n"
        # --- Refined Prompt ---
        prompt = self.prompt_library.evaluate_image_relevance.format(
            attachment_context=attachment_context,
            user_request=user_request,
            image_path=image_path
        )
        searchfacts.image_path_being_evaluated = image_path
        # --- Invoking Response ---
        response = self.llm.invoke(prompt=prompt, temperature=0.01, searchfacts=searchfacts).strip().lower()
        # --- Getting Boolean ---
        if "yes" in response.lower() or "no" in response.lower():
            return "yes" in response
        else:
            return True

    def llm_synthesize_results(self, searchfacts, target_column=None):
        # --- Context Preparation ---
        from datetime import datetime
        date_time = datetime.now().strftime("%#I:%M %p, %d %B %Y")
        attachment_context = f"CONTEXT FROM ATTACHMENT:\n{searchfacts.attachment_context_string}\n" if searchfacts.attachment_context_string else ""
        user_request = f"USER'S REQUEST:\n'{searchfacts.msg}'" if searchfacts.msg else "USER'S REQUEST: The user did not provide a specific prompt; focus on their attachment.\n"
        relevant_chunks = [r['documents'] for r in searchfacts.text_search_results]
        joiner_string = "\n---\n"
        formatted_chunks = f"{joiner_string.join(relevant_chunks)}" if relevant_chunks else "No text results found; focus on the images."
        database_results = f"DATABASE SEARCH RESULTS:\n{formatted_chunks}\n"
        # --- Refined Prompt ---
        prompt = self.prompt_library.synthesize_results.format(
            date_time=date_time,
            user_request=user_request,
            attachment_context=attachment_context,
            database_results=database_results
        )
        # --- Stream Results ---
        threading.Thread(
            target=self.stream_llm_response, 
            args=(prompt, searchfacts, target_column),
            daemon=True).start()

    def create_default_config(self, file_path):
        """Creates a default config.json file with all settings."""
        import json
        config_data = {
            "credentials_path": "credentials.json",
            "target_directory": "C:\\Users\\user\\Documents",
            "text_model_name": "BAAI/bge-small-en-v1.5",
            "image_model_name": "clip-ViT-B-32",
            "use_cuda": True,
            "batch_size": 16,
            "chunk_size": 200,
            "chunk_overlap": 0,
            "max_seq_length": 512,
            "mmr_lambda": 0.5,
            "mmr_alpha": 0.5,
            "search_multiplier": 5,
            "log_messages": True,
            "ai_mode": False,
            "show_buttons": True,
            "llm_filter_results": False,
            "llm_backend": "LM Studio",
            "lms_model_name": "unsloth/gemma-3-4b-it",
            "openai_model_name": "gpt-4.1",
            "max_results": 6,
            "text_search_prefix": "Represent this sentence for searching relevant passages: ",
            "query_multiplier": 4,
            "max_attachment_size": 1000,
            "n_attachment_chunks": 3,
            "system_prompt": "You are a personal search assistant, made to turn user prompts into accurate and relevant search results, using information from the user's database. Special instructions: ",
            "special_instructions": "Your persona is that of an analytical and definitive guide. You explain all topics with a formal, structured, and declarative tone. You frequently use simple, structured analogies to illustrate relationships and often frame your responses with short, philosophical aphorisms.",
            "generate_queries_prompt": "{system_prompt}\n\n{user_request}\n{attachment_context}\nBased on the user's prompt, generate {n} creative search queries that could retrieve relevant {content} to answer the user. These queries will go into a semantic and lexical search algorithm to retreive relevant {content} from the user's database. The queries should be broad enough to find a variety of related items. These queries will search a somewhat small and personal database (that is, the user's hard drive). Respond with a plain list with no supporting text or markdown, and do not use internet search engine syntax.{reminder_suffix}",
            "evaluate_text_relevance_prompt": "{system_prompt}\n\n{attachment_context}\n{user_request}\nDocument excerpt to evaluate:\n\"{chunk}\"\n\nIs this excerpt worth keeping? Respond only with YES or NO.\n\nRelevance is the most important thing. Does the snippet connect to the user's request?\n\nIf the excerpt is gibberish, respond with NO.\n\n(Again: respond only with YES or NO.)",
            "evaluate_image_relevance_prompt": "{system_prompt}\n\n{attachment_context}\n{user_request}\nIs the provided image worth keeping? Respond only with YES or NO.\n\nRelevance is the most important thing. Does the photo connect to the user's request?\n\nIf the image is blank, corrupted, or unreadable, respond with NO.\n\nImage file path: {image_path}\n\nIf the user's query has an exact match within the file path, respond with YES.\n\n(Again: respond only with YES or NO.)",
            "synthesize_results_prompt": "{system_prompt}\n\nIt is {date_time}.\n\n{user_request}\n{attachment_context}\n{database_results}\n**Your Task:**\nBased exclusively on the information provided above, write a concise and helpful response. Your primary goal is to synthesize the information to **guide the user towards what they want**.\n\n**Instructions:**\n- The text search results are **snippets** from larger documents and may be incomplete.\n- Do **not assume or guess** the author of a document unless the source text makes it absolutely clear.\n- The documents don't have timestamps; don't assume the age of a document unless the source text makes it absolutely clear.\n- Cite every piece of information you use from the search results with its source, like so: (source_name).\n- If the provided search results are not relevant to the user's request, state that you could not find any relevant information.\n- Use markdown formatting (e.g., bolding, bullet points) to make the response easy to read.\n- If there are images, make sure to consider them for your response.",
            "search_prefix": "Represent this sentence for searching relevant passages:"
        }

        # Disables Drive sync functions
        if is_final_microsoft_store_product:
            # config_data['use_drive'] = False
            # Hide that this is even an option, use config.get("use_drive", False)
            ...
        else:
            config_data['use_drive'] = True

        # Write the dictionary to the specified file path
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                # Use json.dump() to write data to the file
                # indent=4 makes the file human-readable (like your example)
                json.dump(config_data, f, indent=4)
            self.log(f"Successfully created default config at: {file_path}")
        except Exception as e:
            self.log(f"[ERROR] Error creating config file: {e}")

    def _build_welcome_overlay(self):
        """Creates the Flet controls for the welcome screen overlay."""
        
        # We need a text control to display the selected path
        # This is defined here but will be assigned to self.welcome_dir_path_text
        # in the backend_worker logic if the overlay is shown.
        welcome_dir_path_text = ft.Text(
            "No directory selected", 
            italic=True, 
            size=8,
            color=ft.Colors.ON_SURFACE_VARIANT
        )

        welcome_content = ft.Container(
            content=ft.Column(
                [
                    ft.Text("Welcome to IntelliSearch AI!", size=28, weight="bold", text_align="center"),
                    ft.Column(
                        [
                            ft.Row(
                                [
                                    ft.Text("1. Select your Sync Directory", weight="bold"),
                                    welcome_dir_path_text, # This is the control to update
                                ],
                                alignment=ft.MainAxisAlignment.SPACE_BETWEEN
                            ),
                            ft.Text(
                                "This is the main folder (like 'My Documents') where your files are. IntelliSearch AI will scan it so you can search it.",
                                size=12,
                            ),
                            ft.ElevatedButton(
                                "Select Directory",
                                icon=ft.Icons.FOLDER_OPEN_ROUNDED,
                                on_click=self._welcome_select_directory,
                                expand=True,
                                color=ft.Colors.AMBER_700
                            ),
                        ],
                        spacing=5
                    ),
                    ft.Divider(),
                    ft.Column(
                        [
                            ft.Text("2. Sync Your Files", weight="bold"),
                            ft.Text(
                                "Click the 'Sync Directory' button in the top bar. A full sync can take a long time, but it's worth it.",
                                size=12,
                            ),
                        ],
                        spacing=5
                    ),
                    ft.Divider(),
                    ft.Column(
                        [
                            ft.Text("3. Search!", weight="bold"),
                            ft.Text(
                                "Once synced, use the search bar to find anything. Toggle 'AI Mode' for summaries and insights from an LLM.",
                                size=12,
                            ),
                        ],
                        spacing=5
                    ),
                    ft.Divider(),
                    ft.ElevatedButton(
                        "Get Started   ",  # Spaces for padding
                        icon=ft.Icons.CHECK_ROUNDED,
                        on_click=self._welcome_dismiss,
                        expand=True,
                        bgcolor=ft.Colors.PRIMARY,
                        color=ft.Colors.ON_PRIMARY,
                    )
                ],
                spacing=15,
                scroll=ft.ScrollMode.ADAPTIVE,
                width=450,
                horizontal_alignment=ft.CrossAxisAlignment.START
            ),
            border_radius=20,
            padding=40,
            bgcolor=ft.Colors.BLACK
        )
        
        # This is the main overlay container (dims/blurs the background)
        overlay_container = ft.Container(
            content=welcome_content,
            alignment=ft.alignment.Alignment(0, 0),
            blur=(10, 10),
            visible=False,  # Start hidden
            expand=True,
            bgcolor=ft.Colors.with_opacity(0.3, ft.Colors.BLACK) # Dimming effect
        )
        
        return overlay_container

    def _welcome_select_directory(self, e):
        """Handler for the welcome screen's directory picker."""
        # We re-use the *settings* picker dialog
        self.settings_dir_picker.get_directory_path(
            dialog_title="Select Sync Directory"
        )
        # The result will be handled by your modified `on_settings_pick_result`

    def _welcome_dismiss(self, e):
        """Hides the welcome overlay."""
        self.welcome_overlay.visible = False
        self.user_input.focus()
        self.page.update()

class Prompter:
    """Manages the creation of all complex LLM prompts."""
    def __init__(self, config: dict):
        from langchain_core.prompts import PromptTemplate

        system_prompt = config['system_prompt'] + config.get('special_instructions', 'None')
        # print(system_prompt)
        # llm_generate_queries
        self.generate_queries = PromptTemplate(
            input_variables=["system_prompt", "user_request", "attachment_context", "n", "content", "reminder_suffix"],
            template=config['generate_queries_prompt']
        ).partial(system_prompt=system_prompt)

        # llm_evaluate_text_relevance
        self.evaluate_text_relevance = PromptTemplate(
            input_variables=["system_prompt", "attachment_context", "user_request", "chunk"],
            template=config['evaluate_text_relevance_prompt']
        ).partial(system_prompt=system_prompt)

        # llm_evaluate_image_relevance
        self.evaluate_image_relevance = PromptTemplate(
            input_variables=["system_prompt", "attachment_context", "user_request", "image_path"],
            template=config['evaluate_image_relevance_prompt']
        ).partial(system_prompt=system_prompt)

        # llm_synthesize_results
        self.synthesize_results = PromptTemplate(
            input_variables=["system_prompt", "date_time", "user_request", "attachment_context", "database_results"],
            template=config['synthesize_results_prompt']
        ).partial(system_prompt=system_prompt)

def main(page: ft.Page):
    # Supress ALL print messages for final product
    if is_final_microsoft_store_product:
        sys.stdout = open(os.devnull, 'w', encoding='utf-8')
        sys.stderr = open(os.devnull, 'w', encoding='utf-8')

    page.visible = False
    App(page)

if __name__ == "__main__":
    from multiprocessing import freeze_support
    freeze_support()
    
    # Now, it's safe to run the app
    ft.app(target=main)